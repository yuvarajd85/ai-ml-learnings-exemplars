{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Machine Learning Visualizations\nThis notebook demonstrates key ML concepts through visualizations:\n- Gradient Descent\n- Activation Functions (Sigmoid, ReLU, Softmax)\n- Linear Regression\n- SVM Decision Boundary"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n# 1. Gradient Descent Visualization\ndef gradient_descent_demo():\n    def loss_function(x): return x**2\n    def gradient(x): return 2*x\n\n    x_vals = [5]\n    learning_rate = 0.1\n    for _ in range(20):\n        grad = gradient(x_vals[-1])\n        x_vals.append(x_vals[-1] - learning_rate * grad)\n\n    x_range = np.linspace(-6, 6, 100)\n    y_range = loss_function(x_range)\n\n    plt.plot(x_range, y_range, label='Loss Function: J(x) = x\u00b2')\n    plt.scatter(x_vals, [loss_function(x) for x in x_vals], color='red')\n    plt.title(\"Gradient Descent on J(x) = x\u00b2\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"J(x)\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\ngradient_descent_demo()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 2. Sigmoid Function\ndef sigmoid_plot():\n    x = np.linspace(-10, 10, 100)\n    sigmoid = 1 / (1 + np.exp(-x))\n    plt.plot(x, sigmoid)\n    plt.title(\"Sigmoid Activation Function\")\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Output\")\n    plt.grid(True)\n    plt.show()\n\nsigmoid_plot()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 3. ReLU Function\ndef relu_plot():\n    x = np.linspace(-10, 10, 100)\n    relu = np.maximum(0, x)\n    plt.plot(x, relu)\n    plt.title(\"ReLU Activation Function\")\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Output\")\n    plt.grid(True)\n    plt.show()\n\nrelu_plot()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 4. Softmax Function\ndef softmax_plot():\n    x = np.linspace(-2, 2, 100)\n    logits = np.vstack([x, x + 1, x - 1])\n    exps = np.exp(logits)\n    softmax = exps / np.sum(exps, axis=0)\n    for i in range(3):\n        plt.plot(x, softmax[i], label=f'Class {i}')\n    plt.title(\"Softmax Outputs for 3 Classes\")\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Probability\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nsoftmax_plot()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 5. Linear Regression\ndef linear_regression_demo():\n    np.random.seed(42)\n    X = np.random.rand(100, 1) * 10\n    y = 2.5 * X.flatten() + np.random.randn(100) * 2\n\n    model = LinearRegression()\n    model.fit(X, y)\n    y_pred = model.predict(X)\n\n    mse = mean_squared_error(y, y_pred)\n    r2 = r2_score(y, y_pred)\n\n    plt.scatter(X, y, label='Actual')\n    plt.plot(X, y_pred, color='red', label='Predicted')\n    plt.title(f\"Linear Regression\\nMSE={mse:.2f}, R\u00b2={r2:.2f}\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"y\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nlinear_regression_demo()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 6. SVM Decision Boundary\ndef svm_decision_boundary():\n    X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n                               n_informative=2, n_clusters_per_class=1, random_state=42)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    model = SVC(kernel='linear')\n    model.fit(X, y)\n\n    h = .02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n    plt.title(\"SVM Decision Boundary\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.grid(True)\n    plt.show()\n\nsvm_decision_boundary()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Additional ML Visualizations\n\nThis section includes:\n- KMeans Clustering\n- Entropy in Decision Trees\n- PCA (Principal Component Analysis)\n- KL Divergence"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# KMeans Clustering Visualization\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\ndef kmeans_clustering_demo():\n    X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n    kmeans = KMeans(n_clusters=4, random_state=0)\n    y_kmeans = kmeans.fit_predict(X)\n\n    plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n    centers = kmeans.cluster_centers_\n    plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.75, marker='X')\n    plt.title(\"KMeans Clustering with 4 Clusters\")\n    plt.grid(True)\n    plt.show()\n\nkmeans_clustering_demo()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Entropy Plot for Decision Tree Splits\ndef entropy(p):\n    return -p*np.log2(p) - (1 - p)*np.log2(1 - p)\n\ndef entropy_plot():\n    p = np.linspace(0.01, 0.99, 100)\n    e = entropy(p)\n    plt.plot(p, e)\n    plt.title(\"Entropy of Binary Split\")\n    plt.xlabel(\"Probability p\")\n    plt.ylabel(\"Entropy H(p)\")\n    plt.grid(True)\n    plt.show()\n\nentropy_plot()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# PCA Visualization\nfrom sklearn.decomposition import PCA\n\ndef pca_demo():\n    X, y = make_classification(n_samples=300, n_features=5, n_informative=2, n_redundant=0, random_state=42)\n    X = StandardScaler().fit_transform(X)\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X)\n\n    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n    plt.title(\"PCA Projection to 2D\")\n    plt.xlabel(\"Principal Component 1\")\n    plt.ylabel(\"Principal Component 2\")\n    plt.grid(True)\n    plt.show()\n\npca_demo()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# KL Divergence Visualization\ndef kl_divergence_demo():\n    p = np.linspace(0.01, 0.99, 100)\n    q = 0.5\n    kl = p * np.log(p / q) + (1 - p) * np.log((1 - p) / (1 - q))\n\n    plt.plot(p, kl)\n    plt.title(\"KL Divergence between Bernoulli(p) and Bernoulli(0.5)\")\n    plt.xlabel(\"p\")\n    plt.ylabel(\"D_KL(p || 0.5)\")\n    plt.grid(True)\n    plt.show()\n\nkl_divergence_demo()"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 2}