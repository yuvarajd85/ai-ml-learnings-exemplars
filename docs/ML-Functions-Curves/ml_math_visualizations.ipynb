{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Machine Learning Visualizations\nThis notebook demonstrates key ML concepts through visualizations:\n- Gradient Descent\n- Activation Functions (Sigmoid, ReLU, Softmax)\n- Linear Regression\n- SVM Decision Boundary"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n# 1. Gradient Descent Visualization\ndef gradient_descent_demo():\n    def loss_function(x): return x**2\n    def gradient(x): return 2*x\n\n    x_vals = [5]\n    learning_rate = 0.1\n    for _ in range(20):\n        grad = gradient(x_vals[-1])\n        x_vals.append(x_vals[-1] - learning_rate * grad)\n\n    x_range = np.linspace(-6, 6, 100)\n    y_range = loss_function(x_range)\n\n    plt.plot(x_range, y_range, label='Loss Function: J(x) = x\u00b2')\n    plt.scatter(x_vals, [loss_function(x) for x in x_vals], color='red')\n    plt.title(\"Gradient Descent on J(x) = x\u00b2\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"J(x)\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n\ngradient_descent_demo()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 2. Sigmoid Function\ndef sigmoid_plot():\n    x = np.linspace(-10, 10, 100)\n    sigmoid = 1 / (1 + np.exp(-x))\n    plt.plot(x, sigmoid)\n    plt.title(\"Sigmoid Activation Function\")\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Output\")\n    plt.grid(True)\n    plt.show()\n\nsigmoid_plot()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 3. ReLU Function\ndef relu_plot():\n    x = np.linspace(-10, 10, 100)\n    relu = np.maximum(0, x)\n    plt.plot(x, relu)\n    plt.title(\"ReLU Activation Function\")\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Output\")\n    plt.grid(True)\n    plt.show()\n\nrelu_plot()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 4. Softmax Function\ndef softmax_plot():\n    x = np.linspace(-2, 2, 100)\n    logits = np.vstack([x, x + 1, x - 1])\n    exps = np.exp(logits)\n    softmax = exps / np.sum(exps, axis=0)\n    for i in range(3):\n        plt.plot(x, softmax[i], label=f'Class {i}')\n    plt.title(\"Softmax Outputs for 3 Classes\")\n    plt.xlabel(\"Input\")\n    plt.ylabel(\"Probability\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nsoftmax_plot()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 5. Linear Regression\ndef linear_regression_demo():\n    np.random.seed(42)\n    X = np.random.rand(100, 1) * 10\n    y = 2.5 * X.flatten() + np.random.randn(100) * 2\n\n    model = LinearRegression()\n    model.fit(X, y)\n    y_pred = model.predict(X)\n\n    mse = mean_squared_error(y, y_pred)\n    r2 = r2_score(y, y_pred)\n\n    plt.scatter(X, y, label='Actual')\n    plt.plot(X, y_pred, color='red', label='Predicted')\n    plt.title(f\"Linear Regression\\nMSE={mse:.2f}, R\u00b2={r2:.2f}\")\n    plt.xlabel(\"X\")\n    plt.ylabel(\"y\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nlinear_regression_demo()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 6. SVM Decision Boundary\ndef svm_decision_boundary():\n    X, y = make_classification(n_samples=200, n_features=2, n_redundant=0,\n                               n_informative=2, n_clusters_per_class=1, random_state=42)\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n\n    model = SVC(kernel='linear')\n    model.fit(X, y)\n\n    h = .02\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n    plt.title(\"SVM Decision Boundary\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.grid(True)\n    plt.show()\n\nsvm_decision_boundary()"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 2}