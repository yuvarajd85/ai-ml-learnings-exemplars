{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3828f2-ca43-40d9-aa6f-315ae75a0ab0",
   "metadata": {},
   "source": [
    "# Deep LEarning Fundamentals - Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5231854-efd9-42b6-bf1c-b7df41c4ea5f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **1. Deciding the Number of Neurons**\n",
    "\n",
    "### **A. Output Layer**\n",
    "\n",
    "The number of neurons here is **fixed by the problem type**:\n",
    "\n",
    "| **Problem Type**               | **Output Neurons**    | **Reason**                                   |\n",
    "| ------------------------------ | --------------------- | -------------------------------------------- |\n",
    "| **Regression (single target)** | 1                     | Predict a continuous value                   |\n",
    "| **Regression (multi-target)**  | One neuron per target | Predict multiple continuous values           |\n",
    "| **Binary classification**      | 1                     | Output probability (sigmoid)                 |\n",
    "| **Multiclass classification**  | `n_classes`           | One neuron per class (softmax)               |\n",
    "| **Multi-label classification** | `n_labels`            | Each neuron predicts independent probability |\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Hidden Layers**\n",
    "\n",
    "Thereâ€™s no universal formula, but here are practical guidelines:\n",
    "\n",
    "#### **Rule-of-thumb approaches**\n",
    "\n",
    "1. **Between input and output sizes**:\n",
    "\n",
    "   * Start with a size somewhere between `input_features` and `output_neurons`.\n",
    "   * E.g., if input = 50 features, output = 1, start with 32 or 64 neurons in the first hidden layer.\n",
    "2. **Pyramid/Tapering structure**:\n",
    "\n",
    "   * Larger first layer â†’ gradually smaller layers (e.g., 128 â†’ 64 â†’ 32 â†’ output).\n",
    "3. **Power of 2 sizes**:\n",
    "\n",
    "   * Easy for vectorized computation (32, 64, 128, etc.).\n",
    "\n",
    "#### **Data complexity approach**\n",
    "\n",
    "* **Simple tabular regression/classification** â†’ start with 1â€“3 hidden layers of 16â€“128 neurons.\n",
    "* **Image classification (MLP)** â†’ often needs hundreds to thousands of neurons in initial layers.\n",
    "* **Sequence modeling** (RNN/Transformer feedforward) â†’ 256â€“2048 neurons in dense parts.\n",
    "\n",
    "ðŸ“Œ **Warning**: Too many neurons â†’ overfitting, longer training. Too few â†’ underfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Deciding the Number of Layers**\n",
    "\n",
    "### **Guidelines**\n",
    "\n",
    "1. **Shallow networks (1â€“2 hidden layers)**:\n",
    "\n",
    "   * Work well for simple regression/classification on tabular data.\n",
    "2. **Moderately deep networks (3â€“8 layers)**:\n",
    "\n",
    "   * Suitable for moderately complex patterns (structured + unstructured data).\n",
    "3. **Deep networks (>8 layers)**:\n",
    "\n",
    "   * Used in computer vision (CNNs), NLP (Transformers), generative models.\n",
    "\n",
    "ðŸ“Œ **Practical advice**:\n",
    "\n",
    "* Start simple â†’ increase depth only if the model underfits.\n",
    "* Use **validation loss** as a guide: if adding layers reduces validation loss, itâ€™s helping; if not, itâ€™s adding complexity without benefit.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Deciding the Activation Functions**\n",
    "\n",
    "### **A. Hidden Layers**\n",
    "\n",
    "* **Default choice**: **ReLU**\n",
    "\n",
    "  * Fast, works well for most deep networks.\n",
    "* **Leaky ReLU / Parametric ReLU**:\n",
    "\n",
    "  * If worried about **dead ReLUs** (neurons stuck at 0 output).\n",
    "* **Tanh**:\n",
    "\n",
    "  * If inputs are zero-centered and small-scale networks.\n",
    "* **GELU**:\n",
    "\n",
    "  * Popular in Transformer-based models.\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Output Layer** (based on problem type)\n",
    "\n",
    "| **Problem Type**                                   | **Output Activation**                        | **Reason**                                  |\n",
    "| -------------------------------------------------- | -------------------------------------------- | ------------------------------------------- |\n",
    "| **Regression (unbounded)**                         | None (linear activation)                     | Allows predicting any real number           |\n",
    "| **Regression (bounded, e.g., 0â€“1)**                | Sigmoid                                      | Constrains output                           |\n",
    "| **Binary classification**                          | Sigmoid                                      | Outputs probability for one class           |\n",
    "| **Multiclass classification**                      | Softmax                                      | Converts logits to probability distribution |\n",
    "| **Multi-label classification**                     | Sigmoid (per label)                          | Each label independent                      |\n",
    "| **Sequence-to-sequence (e.g., language modeling)** | Softmax at each time step                    | Predicts token probabilities                |\n",
    "| **Autoencoders (decoder output)**                  | Sigmoid (if normalized 0â€“1) / Tanh (-1 to 1) | Matches data scale                          |\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick PyTorch Reference Table**\n",
    "\n",
    "| **Problem**                | **Hidden Layer Activation** | **Output Activation** | **Loss Function**                                 |\n",
    "| -------------------------- | --------------------------- | --------------------- | ------------------------------------------------- |\n",
    "| Regression (any real)      | ReLU / Leaky ReLU           | None                  | `MSELoss()`                                       |\n",
    "| Regression (0â€“1)           | ReLU / Leaky ReLU           | Sigmoid               | `MSELoss()` / `BCELoss()`                         |\n",
    "| Binary Classification      | ReLU / Leaky ReLU           | Sigmoid               | `BCEWithLogitsLoss()` *(omit Sigmoid in forward)* |\n",
    "| Multiclass Classification  | ReLU / Leaky ReLU           | Softmax               | `CrossEntropyLoss()` *(omit Softmax in forward)*  |\n",
    "| Multi-label Classification | ReLU / Leaky ReLU           | Sigmoid               | `BCEWithLogitsLoss()`                             |\n",
    "| Sequence Modeling          | ReLU / GELU                 | Softmax               | `CrossEntropyLoss()`                              |\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Architecture Choices**\n",
    "\n",
    "**Binary classification (tabular)**\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(30, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),   # output neuron\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "```\n",
    "\n",
    "**Multiclass classification (10 classes)**\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)  # logits\n",
    ")  # Softmax applied in loss\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df246f-f4bc-46eb-b25c-5486b7647c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
