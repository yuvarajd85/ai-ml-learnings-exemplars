{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31d36d2-04f0-4de7-89c1-f1f2257444a8",
   "metadata": {},
   "source": [
    "# Fundamentals of Deep LEarning - PArt 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b0d00-debc-4c47-96a7-a916282108e2",
   "metadata": {},
   "source": [
    "## **1. Why Deep Learning?**\n",
    "\n",
    "Deep Learning (DL) is a subfield of Machine Learning (ML) that uses **multi-layered artificial neural networks** to model complex patterns in data.\n",
    "\n",
    "### **Key Reasons for Popularity**\n",
    "\n",
    "* **Handles raw, unstructured data**: Images, audio, text — without heavy manual feature engineering.\n",
    "* **Automatic feature extraction**: Learns hierarchical features directly from data.\n",
    "* **Scales with data and compute**: Performance improves significantly with more data and GPU/TPU power.\n",
    "* **Breakthrough results**: Outperformed traditional ML in computer vision, NLP, speech recognition, recommendation systems, etc.\n",
    "* **End-to-end learning**: Goes from raw input → prediction without intermediate handcrafted features.\n",
    "\n",
    "📌 **Real-world examples:**\n",
    "\n",
    "* Face recognition in smartphones\n",
    "* Google Translate’s neural machine translation\n",
    "* ChatGPT and generative AI models\n",
    "* Autonomous vehicle perception systems\n",
    "\n",
    "---\n",
    "\n",
    "## **2. DL vs ML: Technical Differences and Use Cases**\n",
    "\n",
    "| **Aspect**              | **Machine Learning (ML)**                                | **Deep Learning (DL)**                                           |\n",
    "| ----------------------- | -------------------------------------------------------- | ---------------------------------------------------------------- |\n",
    "| **Data requirement**    | Works well with small/medium datasets                    | Requires large datasets for good performance                     |\n",
    "| **Feature engineering** | Manual feature extraction is crucial                     | Learns features automatically                                    |\n",
    "| **Model complexity**    | Simpler models (e.g., linear regression, decision trees) | Multi-layer neural networks with millions/billions of parameters |\n",
    "| **Computation**         | Can run on CPUs easily                                   | Often requires GPUs/TPUs                                         |\n",
    "| **Interpretability**    | Easier to interpret                                      | More of a “black box”                                            |\n",
    "| **Performance**         | Can saturate on complex tasks                            | Can scale performance with data and depth                        |\n",
    "\n",
    "**Use Cases**\n",
    "\n",
    "* **ML**: Credit scoring, churn prediction, time series forecasting (small data), recommendation with tabular data\n",
    "* **DL**: Image classification, NLP (chatbots, translation), audio transcription, large-scale recommender systems\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Steps Involved in a Deep Learning Workflow**\n",
    "\n",
    "1. **Define the Problem**\n",
    "\n",
    "   * Is it classification, regression, segmentation, generation, etc.?\n",
    "\n",
    "2. **Gather & Prepare Data**\n",
    "\n",
    "   * Collect datasets (images, text, audio, etc.).\n",
    "   * Split into training, validation, test sets.\n",
    "   * Preprocess (normalization, tokenization, data augmentation).\n",
    "\n",
    "3. **Choose a Model Architecture**\n",
    "\n",
    "   * CNNs for images, RNN/LSTM/Transformers for sequences, GANs for generative tasks.\n",
    "\n",
    "4. **Define the Loss Function**\n",
    "\n",
    "   * E.g., Cross-Entropy Loss for classification, MSE for regression.\n",
    "\n",
    "5. **Select the Optimizer**\n",
    "\n",
    "   * SGD, Adam, RMSprop — update weights to minimize loss.\n",
    "\n",
    "6. **Train the Model**\n",
    "\n",
    "   * Forward pass → compute loss → backward pass (backpropagation) → update weights.\n",
    "\n",
    "7. **Validate & Tune Hyperparameters**\n",
    "\n",
    "   * Learning rate, batch size, number of layers, dropout rate.\n",
    "\n",
    "8. **Test the Model**\n",
    "\n",
    "   * Measure generalization on unseen data.\n",
    "\n",
    "9. **Deploy**\n",
    "\n",
    "   * Package the model into an application, API, or edge device.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Popular Frameworks for Deep Learning**\n",
    "\n",
    "| Framework      | Language      | Key Features                                                 | Popular Use                          |\n",
    "| -------------- | ------------- | ------------------------------------------------------------ | ------------------------------------ |\n",
    "| **TensorFlow** | Python, C++   | Large ecosystem, production-ready, integrates with Keras     | Google-scale deployments             |\n",
    "| **PyTorch**    | Python        | Dynamic computation graphs, easy to debug, research-friendly | Academic research, production (Meta) |\n",
    "| **Keras**      | Python        | High-level API (can run on TensorFlow, Theano, CNTK)         | Fast prototyping                     |\n",
    "| **JAX**        | Python        | Autograd + XLA compilation for speed                         | High-performance research            |\n",
    "| **MXNet**      | Python, Scala | Efficient distributed training                               | AWS SageMaker backend                |\n",
    "\n",
    "📌 **Current trend**: PyTorch dominates research; TensorFlow/Keras still strong in production.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. What are Neurons in Deep Learning?**\n",
    "\n",
    "A **neuron** is the basic computational unit in a neural network — inspired by biological neurons.\n",
    "\n",
    "### **Structure**\n",
    "\n",
    "* **Inputs** ($x_1, x_2, ..., x_n$)\n",
    "* **Weights** ($w_1, w_2, ..., w_n$) → determines importance of each input\n",
    "* **Bias** ($b$) → shifts activation threshold\n",
    "* **Summation** → $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$\n",
    "* **Activation function** → non-linear transformation (ReLU, sigmoid, tanh, etc.)\n",
    "\n",
    "📌 **Mathematical representation**:\n",
    "\n",
    "$$\n",
    "y = \\phi \\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n",
    "$$\n",
    "\n",
    "where $\\phi$ is the activation function.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. What Do Neurons Learn and How Do They Learn?**\n",
    "\n",
    "### **What they learn**\n",
    "\n",
    "* In early layers → **low-level patterns** (edges, curves in images; word associations in text).\n",
    "* In deeper layers → **high-level concepts** (faces, objects, sentence meaning).\n",
    "\n",
    "### **How they learn**\n",
    "\n",
    "* **Forward pass**: Input flows through the network → prediction is made.\n",
    "* **Loss computation**: Compare prediction to actual label using loss function.\n",
    "* **Backward pass (Backpropagation)**:\n",
    "\n",
    "  * Compute gradients of loss w\\.r.t. weights (∂Loss/∂w).\n",
    "  * Update weights using **gradient descent**:\n",
    "\n",
    "    $$\n",
    "    w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
    "    $$\n",
    "\n",
    "    where $\\eta$ is the learning rate.\n",
    "* Repeat for many **epochs** until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mini Python Example**: A Tiny Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "580fdeb2-68ec-4650-97cb-14033bcfb3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[2.682209e-07]\n",
      " [2.000000e+00]\n",
      " [4.000000e+00]\n",
      " [6.000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dummy dataset\n",
    "X = torch.tensor([[0.0], [1.0], [2.0], [3.0]])\n",
    "y = torch.tensor([[0.0], [2.0], [4.0], [6.0]])  # y = 2x\n",
    "\n",
    "# Simple network\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 5),  # input layer -> hidden layer\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 1)   # hidden -> output layer\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    y_pred = model(X)             # Forward pass\n",
    "    loss = loss_fn(y_pred, y)     # Compute loss\n",
    "    optimizer.zero_grad()         # Reset gradients\n",
    "    loss.backward()               # Backpropagation\n",
    "    optimizer.step()              # Update weights\n",
    "\n",
    "print(\"Predictions:\", model(X).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f291d6-14cd-4a08-b8d2-a5b9a89b7e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
