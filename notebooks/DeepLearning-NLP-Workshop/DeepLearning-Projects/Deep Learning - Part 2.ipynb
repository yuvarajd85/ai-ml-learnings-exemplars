{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7e4855-37df-4a2b-8b29-87b3e5b6bd32",
   "metadata": {},
   "source": [
    "# Deep LEarning Funndamentals - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0848ad-6330-42c2-a925-e08268774fb3",
   "metadata": {},
   "source": [
    "## **7. Activation Functions**\n",
    "\n",
    "### **What are they?**\n",
    "\n",
    "An **activation function** is a mathematical transformation applied to a neuron’s output before passing it to the next layer.\n",
    "\n",
    "**Without them** → a neural network would just be a stack of linear operations, equivalent to a single linear model (no matter how many layers).\n",
    "\n",
    "**With them** → networks can learn **non-linear relationships**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why are they needed?**\n",
    "\n",
    "1. **Non-linearity**: Real-world data often has complex patterns that linear models can’t capture.\n",
    "2. **Feature learning**: Different neurons can activate differently for different patterns.\n",
    "3. **Gradient flow**: Some functions (like ReLU) help avoid vanishing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Activation Functions in DL**\n",
    "\n",
    "| Function       | Formula                                        | Range         | Key Points                                                                         |   |   |\n",
    "| -------------- | ---------------------------------------------- | ------------- | ---------------------------------------------------------------------------------- | - | - |\n",
    "| **Sigmoid**    | $\\sigma(x) = \\frac{1}{1+e^{-x}}$               | (0, 1)        | Smooth, good for probabilities, but suffers from **vanishing gradients** for large | x |   |\n",
    "| **Tanh**       | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1)       | Zero-centered, still suffers from vanishing gradients                              |   |   |\n",
    "| **ReLU**       | $\\text{ReLU}(x) = \\max(0, x)$                  | \\[0, ∞)       | Fast, reduces vanishing gradients, but can cause **dead neurons**                  |   |   |\n",
    "| **Leaky ReLU** | $\\max(0.01x, x)$                               | (-∞, ∞)       | Fixes dead neuron problem                                                          |   |   |\n",
    "| **Softmax**    | $\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$ | (0, 1), sum=1 | Converts logits to probability distribution in multi-class problems                |   |   |\n",
    "| **GELU**       | Smooth approximation of ReLU                   | (-∞, ∞)       | Popular in Transformer models                                                      |   |   |\n",
    "\n",
    "**PyTorch Example:**\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "act1 = nn.ReLU()\n",
    "act2 = nn.Sigmoid()\n",
    "act3 = nn.Tanh()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Optimizers**\n",
    "\n",
    "### **What are they?**\n",
    "\n",
    "Optimizers are algorithms that adjust **model parameters (weights & biases)** to minimize the loss function during training.\n",
    "\n",
    "---\n",
    "\n",
    "### **Function**\n",
    "\n",
    "1. Compute gradients using **backpropagation**.\n",
    "2. Update parameters using gradients, learning rate, and sometimes momentum/other terms.\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Optimizers in PyTorch**\n",
    "\n",
    "| Optimizer                           | Characteristics                                                   | Common Use                              |\n",
    "| ----------------------------------- | ----------------------------------------------------------------- | --------------------------------------- |\n",
    "| **SGD** (`torch.optim.SGD`)         | Simple, stable; can use momentum                                  | Small-to-medium problems, vision models |\n",
    "| **Adam** (`torch.optim.Adam`)       | Adaptive learning rates per parameter                             | Most popular for NLP, CV, tabular       |\n",
    "| **AdamW** (`torch.optim.AdamW`)     | Adam + correct weight decay handling                              | Transformers, large models              |\n",
    "| **RMSprop** (`torch.optim.RMSprop`) | Scales learning rate based on moving average of squared gradients | RNNs, noisy problems                    |\n",
    "| **Adagrad** (`torch.optim.Adagrad`) | Adjusts LR for each parameter based on past gradients             | Sparse data (NLP)                       |\n",
    "\n",
    "**PyTorch Example:**\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Loss Functions in Deep Learning**\n",
    "\n",
    "### **What are they?**\n",
    "\n",
    "A **loss function** measures how far the model’s prediction is from the actual target.\n",
    "Lower loss → better performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Loss Functions in PyTorch**\n",
    "\n",
    "#### **A. For Regression**\n",
    "\n",
    "| Loss                      | PyTorch Class  | Notes                        |\n",
    "| ------------------------- | -------------- | ---------------------------- |\n",
    "| Mean Squared Error (MSE)  | `nn.MSELoss()` | Penalizes larger errors more |\n",
    "| Mean Absolute Error (MAE) | `nn.L1Loss()`  | Robust to outliers           |\n",
    "\n",
    "#### **B. For Binary Classification**\n",
    "\n",
    "| Loss                 | PyTorch Class            | Notes                                            |\n",
    "| -------------------- | ------------------------ | ------------------------------------------------ |\n",
    "| Binary Cross-Entropy | `nn.BCELoss()`           | Use when outputs are probabilities (sigmoid)     |\n",
    "| BCE with Logits      | `nn.BCEWithLogitsLoss()` | More stable numerically (combines sigmoid + BCE) |\n",
    "\n",
    "#### **C. For Multi-class Classification**\n",
    "\n",
    "| Loss                    | PyTorch Class           | Notes                            |\n",
    "| ----------------------- | ----------------------- | -------------------------------- |\n",
    "| Cross-Entropy Loss      | `nn.CrossEntropyLoss()` | Combines `LogSoftmax` + NLL loss |\n",
    "| Negative Log Likelihood | `nn.NLLLoss()`          | Used after `LogSoftmax` output   |\n",
    "\n",
    "#### **D. For Other Tasks**\n",
    "\n",
    "| Task                 | Loss                                        | PyTorch Class |\n",
    "| -------------------- | ------------------------------------------- | ------------- |\n",
    "| Image segmentation   | Dice Loss (custom), `nn.CrossEntropyLoss()` |               |\n",
    "| Embedding similarity | `nn.CosineEmbeddingLoss()`                  |               |\n",
    "| Metric learning      | `nn.TripletMarginLoss()`                    |               |\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick PyTorch Example**\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example: binary classification\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # includes sigmoid internally\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Summary Table**:\n",
    "\n",
    "| **Concept**         | **Purpose**                    | **Example in PyTorch**           |\n",
    "| ------------------- | ------------------------------ | -------------------------------- |\n",
    "| Activation Function | Introduce non-linearity        | `nn.ReLU()`                      |\n",
    "| Optimizer           | Update weights using gradients | `optim.Adam(model.parameters())` |\n",
    "| Loss Function       | Measure prediction error       | `nn.CrossEntropyLoss()`          |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3801a1-24b6-48c2-8b15-1c469b5dac10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
