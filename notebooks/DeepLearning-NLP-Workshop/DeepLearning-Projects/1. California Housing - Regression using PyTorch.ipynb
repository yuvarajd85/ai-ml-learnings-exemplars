{"cells":[{"cell_type":"markdown","id":"2e0550f7-2255-4f89-a285-c0e239163198","metadata":{"id":"2e0550f7-2255-4f89-a285-c0e239163198"},"source":["PyTorch Regression Example with California Housing Dataset\n","==========================================================\n","\n","This script demonstrates how to build, train, and evaluate a simple deep learning regression model using PyTorch. The California Housing dataset is used for predicting house prices based on various features."]},{"cell_type":"code","execution_count":null,"id":"acdcf96f-cdda-4d1e-9f91-c5d15b79ad72","metadata":{"id":"acdcf96f-cdda-4d1e-9f91-c5d15b79ad72","outputId":"6bde1174-4992-44ef-ac44-8360f2ab1fcf"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"]}],"source":["# !pip install torch torchvision torchaudio --quiet\n","# !pip install scikit-learn --quiet\n","# !pip install matplotlib --quiet\n","# !pip install torchviz --quiet"]},{"cell_type":"code","execution_count":1,"id":"2f5ea38e-15d6-4ad7-9ceb-2b226420a3b1","metadata":{"id":"2f5ea38e-15d6-4ad7-9ceb-2b226420a3b1","executionInfo":{"status":"ok","timestamp":1731166152726,"user_tz":-330,"elapsed":9168,"user":{"displayName":"Prashant Sahu","userId":"04632843584929853772"}}},"outputs":[],"source":["# Import necessary libraries\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":2,"id":"e778a822-330c-4a86-a6ae-184c9f241fa6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e778a822-330c-4a86-a6ae-184c9f241fa6","executionInfo":{"status":"ok","timestamp":1731166152728,"user_tz":-330,"elapsed":19,"user":{"displayName":"Prashant Sahu","userId":"04632843584929853772"}},"outputId":"1fdfd887-096a-482d-8843-d2607e67b47b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":2}],"source":["# Check if GPU is available, if not, use CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":3,"id":"4ba4f6c0-4355-4729-b0a9-281fdf1e3b70","metadata":{"id":"4ba4f6c0-4355-4729-b0a9-281fdf1e3b70","executionInfo":{"status":"ok","timestamp":1731166165559,"user_tz":-330,"elapsed":15,"user":{"displayName":"Prashant Sahu","userId":"04632843584929853772"}},"outputId":"2a8acb9d-bc2b-4298-b30a-8ec9b228ce68","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Nov  9 15:29:24 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P8              11W /  70W |      3MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","id":"79404830-3f18-4914-8df6-af2d392d2f6c","metadata":{"id":"79404830-3f18-4914-8df6-af2d392d2f6c"},"source":["# 1. Load and preprocess the data"]},{"cell_type":"code","execution_count":null,"id":"04c523d1-5573-4018-b16e-80a0e3858c30","metadata":{"id":"04c523d1-5573-4018-b16e-80a0e3858c30"},"outputs":[],"source":["# Load the California Housing dataset\n","california = fetch_california_housing()"]},{"cell_type":"code","execution_count":null,"id":"864b73c5-3819-4df5-af6e-abf3e9091d3d","metadata":{"id":"864b73c5-3819-4df5-af6e-abf3e9091d3d"},"outputs":[],"source":["# The data is in california.data, target is in california.target\n","X = california.data\n","y = california.target\n","\n","# Convert to pandas DataFrame for easier manipulation (optional)\n","feature_names = california.feature_names\n","df = pd.DataFrame(X, columns=feature_names)\n","df['target'] = y"]},{"cell_type":"code","execution_count":null,"id":"8d67dc25-74b6-4523-84ee-041439e6e4d6","metadata":{"id":"8d67dc25-74b6-4523-84ee-041439e6e4d6"},"outputs":[],"source":["# Split the data into training, validation, and test sets\n","X_trainval, X_test, y_trainval, y_test = train_test_split(\n","    df.iloc[:, :-1], df['target'], test_size=0.2, random_state=42)\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_trainval, y_trainval, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"]},{"cell_type":"code","execution_count":null,"id":"6b7092aa-dd70-4783-b930-4f27bc8ff73f","metadata":{"id":"6b7092aa-dd70-4783-b930-4f27bc8ff73f"},"outputs":[],"source":["# Feature Scaling\n","# Standardize the features using StandardScaler\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_val = scaler.transform(X_val)\n","X_test = scaler.transform(X_test)"]},{"cell_type":"markdown","id":"f8e4eade-619a-4ee9-95c3-5014a352ea36","metadata":{"id":"f8e4eade-619a-4ee9-95c3-5014a352ea36"},"source":["# 2. Create custom Dataset class"]},{"cell_type":"code","execution_count":null,"id":"af1e06e8-0002-475a-9f7c-29722a5bdb49","metadata":{"id":"af1e06e8-0002-475a-9f7c-29722a5bdb49"},"outputs":[],"source":["class CaliforniaHousingDataset(Dataset):\n","    def __init__(self, features, targets):\n","        \"\"\"\n","        Args:\n","            features (numpy.ndarray): Features data.\n","            targets (numpy.ndarray): Target data.\n","        \"\"\"\n","        self.features = torch.tensor(features, dtype=torch.float32)\n","        self.targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1)  # Make it Nx1\n","\n","    def __len__(self):\n","        return len(self.targets)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Args:\n","            idx (int): Index of the data point.\n","        Returns:\n","            (tuple): (feature, target) of the given index.\n","        \"\"\"\n","        return self.features[idx], self.targets[idx]"]},{"cell_type":"code","execution_count":null,"id":"c55c722f-286b-4570-9e12-3cab237a6bb2","metadata":{"id":"c55c722f-286b-4570-9e12-3cab237a6bb2"},"outputs":[],"source":["# Create Dataset objects\n","train_dataset = CaliforniaHousingDataset(X_train, y_train.values)\n","val_dataset = CaliforniaHousingDataset(X_val, y_val.values)\n","test_dataset = CaliforniaHousingDataset(X_test, y_test.values)"]},{"cell_type":"markdown","id":"a907c481-2ecb-49e4-aa14-65cdc34a9a8f","metadata":{"id":"a907c481-2ecb-49e4-aa14-65cdc34a9a8f"},"source":["# 3. Create DataLoaders"]},{"cell_type":"code","execution_count":null,"id":"4b18301c-ab0b-430f-b38f-c0faff520c84","metadata":{"id":"4b18301c-ab0b-430f-b38f-c0faff520c84"},"outputs":[],"source":["batch_size = 64\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)"]},{"cell_type":"markdown","id":"29a80db3-bf8a-482f-a4a4-33350addd242","metadata":{"id":"29a80db3-bf8a-482f-a4a4-33350addd242"},"source":["# 4. Define the Neural Network Model"]},{"cell_type":"code","execution_count":null,"id":"7c640cc8-86c3-4313-b3ae-702141d16420","metadata":{"id":"7c640cc8-86c3-4313-b3ae-702141d16420"},"outputs":[],"source":["class RegressionModel(nn.Module):\n","    def __init__(self, input_size):\n","        \"\"\"\n","        Args:\n","            input_size (int): Number of input features.\n","        \"\"\"\n","        super(RegressionModel, self).__init__()\n","        self.network = nn.Sequential(\n","            nn.Linear(input_size, 64),  # First hidden layer with 64 neurons\n","            nn.ReLU(),\n","            nn.Linear(64, 32),          # Second hidden layer with 32 neurons\n","            nn.ReLU(),\n","            nn.Linear(32, 1)            # Output layer\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the neural network.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor.\n","\n","        Returns:\n","            torch.Tensor: Output tensor.\n","        \"\"\"\n","        return self.network(x)"]},{"cell_type":"code","execution_count":null,"id":"537baad0-daf8-4d23-8ce0-7903574f9238","metadata":{"id":"537baad0-daf8-4d23-8ce0-7903574f9238"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e274c54b-5969-4442-9129-609c651c8489","metadata":{"id":"e274c54b-5969-4442-9129-609c651c8489"},"outputs":[],"source":["# Instantiate the model, define loss function and optimizer\n","input_size = X_train.shape[1]\n","model = RegressionModel(input_size).to(device)"]},{"cell_type":"code","execution_count":null,"id":"a223250b-900c-43d6-8b8b-bcc605cd21fa","metadata":{"id":"a223250b-900c-43d6-8b8b-bcc605cd21fa","outputId":"5b835e9f-3b27-49ad-9343-98c147621936"},"outputs":[{"name":"stdout","output_type":"stream","text":["RegressionModel(\n","  (network): Sequential(\n","    (0): Linear(in_features=8, out_features=64, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=64, out_features=32, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=32, out_features=1, bias=True)\n","  )\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"id":"702cd983-fe98-45b7-8e40-240cb67c1281","metadata":{"id":"702cd983-fe98-45b7-8e40-240cb67c1281","outputId":"32ccea58-0b96-4d57-8785-ac333d27e6c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                   [-1, 64]             576\n","              ReLU-2                   [-1, 64]               0\n","            Linear-3                   [-1, 32]           2,080\n","              ReLU-4                   [-1, 32]               0\n","            Linear-5                    [-1, 1]              33\n","================================================================\n","Total params: 2,689\n","Trainable params: 2,689\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.01\n","Estimated Total Size (MB): 0.01\n","----------------------------------------------------------------\n"]}],"source":["from torchsummary import summary\n","\n","# Get the model summary\n","summary(model, input_size=(input_size,), device=str(device))"]},{"cell_type":"code","execution_count":null,"id":"c81d27b5-ad0f-4726-a968-3ebffc08996f","metadata":{"id":"c81d27b5-ad0f-4726-a968-3ebffc08996f","outputId":"505e5273-28ef-4a02-889b-e5c357f468d9"},"outputs":[{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","RegressionModel                          [64, 1]                   --\n","├─Sequential: 1-1                        [64, 1]                   --\n","│    └─Linear: 2-1                       [64, 64]                  576\n","│    └─ReLU: 2-2                         [64, 64]                  --\n","│    └─Linear: 2-3                       [64, 32]                  2,080\n","│    └─ReLU: 2-4                         [64, 32]                  --\n","│    └─Linear: 2-5                       [64, 1]                   33\n","==========================================================================================\n","Total params: 2,689\n","Trainable params: 2,689\n","Non-trainable params: 0\n","Total mult-adds (M): 0.17\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.05\n","Params size (MB): 0.01\n","Estimated Total Size (MB): 0.06\n","=========================================================================================="]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["from torchinfo import summary\n","\n","# Get the detailed model summary\n","summary(model, input_size=(batch_size, input_size), device=str(device))\n"]},{"cell_type":"markdown","id":"967e15b4-1f29-4c3b-8c61-bb7f60dbf3aa","metadata":{"id":"967e15b4-1f29-4c3b-8c61-bb7f60dbf3aa"},"source":["This will output a detailed summary, including:\n","\n","- Layer hierarchy\n","- Output shapes at each layer\n","- Number of trainable and non-trainable parameters\n","- Total parameters and memory usage\n"]},{"cell_type":"code","execution_count":null,"id":"879cd60e-0d7f-4cf6-a805-c46fe9c872e2","metadata":{"id":"879cd60e-0d7f-4cf6-a805-c46fe9c872e2"},"outputs":[],"source":["# Define a dummy input tensor with the correct input size\n","dummy_input = torch.randn(1, input_size).to(device)\n","\n","# Export the model to an ONNX file\n","torch.onnx.export(\n","    model,               # Your PyTorch model\n","    dummy_input,         # An example input tensor\n","    \"churn_model.onnx\",  # The file name to save the ONNX model\n","    input_names=['input'],   # Name of the input node\n","    output_names=['output'], # Name of the output node\n","    opset_version=11         # ONNX version\n",")\n"]},{"cell_type":"code","execution_count":null,"id":"3d18b3d8-36a3-4aef-8ea2-05514c005c68","metadata":{"id":"3d18b3d8-36a3-4aef-8ea2-05514c005c68"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"8f805b9c-c714-4dcd-abe8-f210bceea609","metadata":{"id":"8f805b9c-c714-4dcd-abe8-f210bceea609"},"outputs":[],"source":["# Define loss function (Mean Squared Error for regression)\n","criterion = nn.MSELoss()\n","\n","# Define optimizer (Adam optimizer)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","id":"e70ecb7f-3c8c-4bb0-a61a-2b4cc5d01687","metadata":{"id":"e70ecb7f-3c8c-4bb0-a61a-2b4cc5d01687"},"source":["# 5. Training Loop"]},{"cell_type":"code","execution_count":null,"id":"53166c83-1450-4e79-8d2b-438245a4ddf7","metadata":{"id":"53166c83-1450-4e79-8d2b-438245a4ddf7","outputId":"14f97e1d-4dc9-44ee-c6a0-9230c1e3c5f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/100], Training Loss: 1.4710, Validation Loss: 1.0558\n","Epoch [2/100], Training Loss: 0.5217, Validation Loss: 0.4435\n","Epoch [3/100], Training Loss: 0.4150, Validation Loss: 0.4255\n","Epoch [4/100], Training Loss: 0.3936, Validation Loss: 0.4101\n","Epoch [5/100], Training Loss: 0.3816, Validation Loss: 0.3927\n","Epoch [6/100], Training Loss: 0.3728, Validation Loss: 0.3776\n","Epoch [7/100], Training Loss: 0.3677, Validation Loss: 0.3746\n","Epoch [8/100], Training Loss: 0.3613, Validation Loss: 0.3824\n","Epoch [9/100], Training Loss: 0.3517, Validation Loss: 0.3704\n","Epoch [10/100], Training Loss: 0.3450, Validation Loss: 0.3840\n","Epoch [11/100], Training Loss: 0.3432, Validation Loss: 0.3565\n","Epoch [12/100], Training Loss: 0.3351, Validation Loss: 0.3689\n","Epoch [13/100], Training Loss: 0.3293, Validation Loss: 0.3709\n","Epoch [14/100], Training Loss: 0.3275, Validation Loss: 0.3675\n","Epoch [15/100], Training Loss: 0.3268, Validation Loss: 0.4201\n","Epoch [16/100], Training Loss: 0.3256, Validation Loss: 0.3823\n","Epoch [17/100], Training Loss: 0.3217, Validation Loss: 0.3574\n","Epoch [18/100], Training Loss: 0.3102, Validation Loss: 0.3253\n","Epoch [19/100], Training Loss: 0.3063, Validation Loss: 0.3530\n","Epoch [20/100], Training Loss: 0.3018, Validation Loss: 0.3249\n","Epoch [21/100], Training Loss: 0.2974, Validation Loss: 0.3154\n","Epoch [22/100], Training Loss: 0.3004, Validation Loss: 0.3577\n","Epoch [23/100], Training Loss: 0.3044, Validation Loss: 0.4417\n","Epoch [24/100], Training Loss: 0.3043, Validation Loss: 0.3264\n","Epoch [25/100], Training Loss: 0.2947, Validation Loss: 0.3441\n","Epoch [26/100], Training Loss: 0.2890, Validation Loss: 0.3036\n","Epoch [27/100], Training Loss: 0.2896, Validation Loss: 0.3135\n","Epoch [28/100], Training Loss: 0.2850, Validation Loss: 0.3076\n","Epoch [29/100], Training Loss: 0.2865, Validation Loss: 0.3080\n","Epoch [30/100], Training Loss: 0.2858, Validation Loss: 0.3087\n","Epoch [31/100], Training Loss: 0.2829, Validation Loss: 0.3154\n","Epoch [32/100], Training Loss: 0.2844, Validation Loss: 0.3311\n","Epoch [33/100], Training Loss: 0.2812, Validation Loss: 0.3054\n","Epoch [34/100], Training Loss: 0.2800, Validation Loss: 0.3533\n","Epoch [35/100], Training Loss: 0.2831, Validation Loss: 0.3085\n","Epoch [36/100], Training Loss: 0.2782, Validation Loss: 0.2941\n","Epoch [37/100], Training Loss: 0.2778, Validation Loss: 0.3415\n","Epoch [38/100], Training Loss: 0.2782, Validation Loss: 0.3130\n","Epoch [39/100], Training Loss: 0.2814, Validation Loss: 0.3771\n","Epoch [40/100], Training Loss: 0.2775, Validation Loss: 0.3242\n","Epoch [41/100], Training Loss: 0.2790, Validation Loss: 0.3352\n","Epoch [42/100], Training Loss: 0.2785, Validation Loss: 0.3101\n","Epoch [43/100], Training Loss: 0.2731, Validation Loss: 0.2948\n","Epoch [44/100], Training Loss: 0.2712, Validation Loss: 0.2920\n","Epoch [45/100], Training Loss: 0.2725, Validation Loss: 0.3161\n","Epoch [46/100], Training Loss: 0.2715, Validation Loss: 0.3588\n","Epoch [47/100], Training Loss: 0.2794, Validation Loss: 0.3286\n","Epoch [48/100], Training Loss: 0.2706, Validation Loss: 0.2968\n","Epoch [49/100], Training Loss: 0.2685, Validation Loss: 0.2867\n","Epoch [50/100], Training Loss: 0.2674, Validation Loss: 0.2960\n","Epoch [51/100], Training Loss: 0.2677, Validation Loss: 0.2894\n","Epoch [52/100], Training Loss: 0.2660, Validation Loss: 0.2965\n","Epoch [53/100], Training Loss: 0.2654, Validation Loss: 0.2850\n","Epoch [54/100], Training Loss: 0.2716, Validation Loss: 0.3147\n","Epoch [55/100], Training Loss: 0.2657, Validation Loss: 0.2896\n","Epoch [56/100], Training Loss: 0.2643, Validation Loss: 0.2899\n","Epoch [57/100], Training Loss: 0.2641, Validation Loss: 0.3088\n","Epoch [58/100], Training Loss: 0.2651, Validation Loss: 0.2899\n","Epoch [59/100], Training Loss: 0.2640, Validation Loss: 0.3118\n","Epoch [60/100], Training Loss: 0.2630, Validation Loss: 0.2984\n","Epoch [61/100], Training Loss: 0.2603, Validation Loss: 0.2855\n","Epoch [62/100], Training Loss: 0.2607, Validation Loss: 0.2848\n","Epoch [63/100], Training Loss: 0.2612, Validation Loss: 0.3067\n","Epoch [64/100], Training Loss: 0.2599, Validation Loss: 0.2806\n","Epoch [65/100], Training Loss: 0.2595, Validation Loss: 0.2912\n","Epoch [66/100], Training Loss: 0.2649, Validation Loss: 0.3280\n","Epoch [67/100], Training Loss: 0.2640, Validation Loss: 0.2831\n","Epoch [68/100], Training Loss: 0.2613, Validation Loss: 0.2795\n","Epoch [69/100], Training Loss: 0.2584, Validation Loss: 0.2806\n","Epoch [70/100], Training Loss: 0.2584, Validation Loss: 0.2800\n","Epoch [71/100], Training Loss: 0.2576, Validation Loss: 0.2823\n","Epoch [72/100], Training Loss: 0.2569, Validation Loss: 0.3005\n","Epoch [73/100], Training Loss: 0.2571, Validation Loss: 0.2950\n","Epoch [74/100], Training Loss: 0.2574, Validation Loss: 0.3046\n","Epoch [75/100], Training Loss: 0.2565, Validation Loss: 0.2872\n","Epoch [76/100], Training Loss: 0.2562, Validation Loss: 0.3076\n","Epoch [77/100], Training Loss: 0.2561, Validation Loss: 0.2941\n","Epoch [78/100], Training Loss: 0.2575, Validation Loss: 0.3231\n","Epoch [79/100], Training Loss: 0.2603, Validation Loss: 0.2815\n","Epoch [80/100], Training Loss: 0.2535, Validation Loss: 0.2799\n","Epoch [81/100], Training Loss: 0.2530, Validation Loss: 0.2827\n","Epoch [82/100], Training Loss: 0.2518, Validation Loss: 0.2735\n","Epoch [83/100], Training Loss: 0.2536, Validation Loss: 0.2778\n","Epoch [84/100], Training Loss: 0.2524, Validation Loss: 0.2749\n","Epoch [85/100], Training Loss: 0.2519, Validation Loss: 0.2764\n","Epoch [86/100], Training Loss: 0.2515, Validation Loss: 0.2875\n","Epoch [87/100], Training Loss: 0.2535, Validation Loss: 0.2971\n","Epoch [88/100], Training Loss: 0.2536, Validation Loss: 0.2789\n","Epoch [89/100], Training Loss: 0.2526, Validation Loss: 0.2992\n","Epoch [90/100], Training Loss: 0.2523, Validation Loss: 0.2808\n","Epoch [91/100], Training Loss: 0.2519, Validation Loss: 0.2889\n","Epoch [92/100], Training Loss: 0.2514, Validation Loss: 0.3046\n","Epoch [93/100], Training Loss: 0.2513, Validation Loss: 0.3033\n","Epoch [94/100], Training Loss: 0.2489, Validation Loss: 0.2907\n","Epoch [95/100], Training Loss: 0.2494, Validation Loss: 0.2868\n","Epoch [96/100], Training Loss: 0.2475, Validation Loss: 0.2777\n","Epoch [97/100], Training Loss: 0.2502, Validation Loss: 0.2787\n","Epoch [98/100], Training Loss: 0.2480, Validation Loss: 0.2816\n","Epoch [99/100], Training Loss: 0.2476, Validation Loss: 0.2723\n","Epoch [100/100], Training Loss: 0.2480, Validation Loss: 0.2861\n"]}],"source":["num_epochs = 100\n","\n","for epoch in range(num_epochs):\n","    # Set model to training mode\n","    model.train()\n","    running_loss = 0.0\n","\n","    for features, targets in train_loader:\n","        # Move data to the device\n","        features = features.to(device)\n","        targets = targets.to(device)\n","\n","        # Zero the gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(features)\n","\n","        # Compute loss\n","        loss = criterion(outputs, targets)\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Update weights\n","        optimizer.step()\n","\n","        # Accumulate loss\n","        running_loss += loss.item() * features.size(0)\n","\n","    # Calculate average loss over the epoch\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    val_running_loss = 0.0\n","\n","    with torch.no_grad():\n","        for features, targets in val_loader:\n","            features = features.to(device)\n","            targets = targets.to(device)\n","\n","            outputs = model(features)\n","            loss = criterion(outputs, targets)\n","            val_running_loss += loss.item() * features.size(0)\n","\n","    val_loss = val_running_loss / len(val_loader.dataset)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")"]},{"cell_type":"markdown","id":"35441c9d-fd78-4f04-a7d2-86d54386e8d1","metadata":{"id":"35441c9d-fd78-4f04-a7d2-86d54386e8d1"},"source":["# 6. Evaluation on Test Set"]},{"cell_type":"code","execution_count":null,"id":"7da2ff72-6efb-4fcb-8a67-e5d87ab7b34c","metadata":{"id":"7da2ff72-6efb-4fcb-8a67-e5d87ab7b34c","outputId":"dd37aa73-444c-4fe5-b97a-22b443ae827a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Loss: 0.2882\n"]}],"source":["# Set model to evaluation mode\n","model.eval()\n","\n","test_running_loss = 0.0\n","\n","with torch.no_grad():\n","    for features, targets in test_loader:\n","        features = features.to(device)\n","        targets = targets.to(device)\n","\n","        outputs = model(features)\n","        loss = criterion(outputs, targets)\n","        test_running_loss += loss.item() * features.size(0)\n","\n","test_loss = test_running_loss / len(test_loader.dataset)\n","\n","print(f\"Test Loss: {test_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"097500b4-7f45-4fe8-b949-39254e84504f","metadata":{"id":"097500b4-7f45-4fe8-b949-39254e84504f"},"outputs":[],"source":["# Optionally, compute additional metrics like R-squared\n","from sklearn.metrics import r2_score"]},{"cell_type":"code","execution_count":null,"id":"4c9d68f0-b5e1-4c94-a0fc-c197d078afb7","metadata":{"id":"4c9d68f0-b5e1-4c94-a0fc-c197d078afb7"},"outputs":[],"source":["# Collect all predictions and true values\n","all_preds = []\n","all_targets = []\n","\n","with torch.no_grad():\n","    for features, targets in test_loader:\n","        features = features.to(device)\n","        targets = targets.to(device)\n","\n","        outputs = model(features)\n","\n","        all_preds.append(outputs.cpu().numpy())\n","        all_targets.append(targets.cpu().numpy())\n","\n","all_preds = np.concatenate(all_preds, axis=0)\n","all_targets = np.concatenate(all_targets, axis=0)"]},{"cell_type":"code","execution_count":null,"id":"aaaa4a73-50f2-4c15-a522-0e545e7d92e9","metadata":{"id":"aaaa4a73-50f2-4c15-a522-0e545e7d92e9","outputId":"1dc9d46f-3439-4394-9c1a-d571d3681ef4"},"outputs":[{"name":"stdout","output_type":"stream","text":["R-squared on Test Set: 0.7801\n"]}],"source":["# Compute R-squared\n","r2 = r2_score(all_targets, all_preds)\n","\n","print(f\"R-squared on Test Set: {r2:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"5dd4f8dc-2940-43bf-9deb-15b4850047a0","metadata":{"id":"5dd4f8dc-2940-43bf-9deb-15b4850047a0"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}