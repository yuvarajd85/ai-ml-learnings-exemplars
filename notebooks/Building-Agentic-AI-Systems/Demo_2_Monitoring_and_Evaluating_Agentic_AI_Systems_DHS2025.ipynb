{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "97677de96f1d47adae6806237d3eaadb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c599791678946e38680b6ec21a1a174",
              "IPY_MODEL_c37f356501b44f2882b64734caf3035b",
              "IPY_MODEL_d52718151c0e4b5fa00b2658f80d69ca"
            ],
            "layout": "IPY_MODEL_94a926a918d3494e969538d694ece805"
          }
        },
        "0c599791678946e38680b6ec21a1a174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_225713878d7145a3aae647104eba23a1",
            "placeholder": "​",
            "style": "IPY_MODEL_e0b4d4670a624088b02624ebb16f3299",
            "value": ""
          }
        },
        "c37f356501b44f2882b64734caf3035b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7922afe2a4584d44bd16ca1802a1f366",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0d14701b8eb4af7ac1dc63cc6e9ed48",
            "value": 1
          }
        },
        "d52718151c0e4b5fa00b2658f80d69ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1634bc906fe4bc389c65469bf8038e7",
            "placeholder": "​",
            "style": "IPY_MODEL_105be6b2b1f04882b32c0071b0688b02",
            "value": " 5/? [00:47&lt;00:00,  9.27s/it]"
          }
        },
        "94a926a918d3494e969538d694ece805": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "225713878d7145a3aae647104eba23a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0b4d4670a624088b02624ebb16f3299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7922afe2a4584d44bd16ca1802a1f366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a0d14701b8eb4af7ac1dc63cc6e9ed48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1634bc906fe4bc389c65469bf8038e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "105be6b2b1f04882b32c0071b0688b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Monitoring and Evaluating Agentic AI Systems with LangGraph & LangSmith\n",
        "\n",
        "This notebook demonstrates how to build, monitor, and evaluate a healthcare utilization review agent using **LangGraph**, **LangSmith**, and **AgentEvals/OpenEvals**.  \n",
        "\n",
        "![](https://i.imgur.com/KOJ58Yg.png)\n",
        "\n",
        "- **Build the Agent**: Create a LangGraph ReAct agent with clinical tools and a structured output format.  \n",
        "- **Tracing & Monitoring**: Enable LangSmith tracing to capture runs, inspect tool calls, and monitor performance with built-in dashboards.  \n",
        "- **Evaluation Dataset**: Prepare and upload test cases with reference outputs and trajectories to LangSmith.  \n",
        "- **Custom Evaluators**: Define three LLM-as-judge evaluators — **Goal Accuracy**, **Goal Reasoning Quality**, and **Trajectory Accuracy**.  \n",
        "- **Run Evaluations**: Execute the agent against the dataset, score results with custom evaluators, and analyze them in LangSmith.  \n",
        "\n",
        "By the end, you will have a complete workflow for developing an agent, monitoring it in production-like settings, and rigorously evaluating its decisions and reasoning steps.  \n"
      ],
      "metadata": {
        "id": "3e_H5Izsos6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n",
        "\n",
        "- LangGraph & LangChain for building Agents\n",
        "- LangSmith, OpenEvals and AgentEvals for monitoring and evaluating Agents"
      ],
      "metadata": {
        "id": "M1jEHIwto3je"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM-bi3UAMp4-",
        "outputId": "0583f0e8-445b-4ce6-e1bf-1370b2da821a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.3.27 langchain-community==0.3.27 langchain-openai==0.3.30 langgraph==0.6.5 langsmith==0.4.14 agentevals==0.0.9 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ytgT0Taut7"
      },
      "source": [
        "## Configure API Keys & Environment\n",
        "\n",
        "Set your OpenAI API key and LangSmith API key and set it in the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apikey"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# OpenAI API Key (for chat & embeddings)\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key (https://platform.openai.com/account/api-keys):\\n\")\n",
        "\n",
        "# Langsmith API Key (for agent tracking and evaluation)\n",
        "if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
        "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key (https://smith.langchain.com):\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set LangSmith Project & Enable Tracing\n",
        "\n",
        "Configure LangSmith to capture and group all agent runs under a specific project with tracing enabled.  \n",
        "\n",
        "- Define a **project name** to organize and filter runs.  \n",
        "- Set `LANGSMITH_TRACING=true` so inputs, outputs, and tool calls are logged.  \n",
        "- Provide your **LangSmith API key** (and endpoint if using a private deployment).  "
      ],
      "metadata": {
        "id": "oK7nCsMiGsaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_NAME = 'dhs-mastering-agents-project'#\"dhs-effectiveagents-demo\""
      ],
      "metadata": {
        "id": "P3yxjJ49Gyju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = PROJECT_NAME"
      ],
      "metadata": {
        "id": "gBuX5ruAMrig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build your Utilization Review AI Agent\n",
        "\n",
        "Assemble a LangGraph tool-using ReAct agent that uses clinical tools and a system prompt to produce a structured utilization-review decision.\n",
        "\n",
        "- Define the **system prompt** and enforce the final answer format: **Decision**, **Reasoning**, **Care recommendation**.\n",
        "- Initialize the **LLM** for tool-use and reasoning.\n",
        "- Register domain tools: **fetch_patient_record**, **match_guideline**, **check_guideline_validity**, **recommend_care_plan**.\n",
        "- Create the agent with `create_react_agent(tools=..., llm=...)` so it can call tools and return the final structured response.\n",
        "\n",
        "![](https://i.imgur.com/s9hSJ6l.png)"
      ],
      "metadata": {
        "id": "X9hHKVhqPE-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Data (Guidelines, Care Plans, Patient Records)\n",
        "\n",
        "This notebook uses **in-memory** Python lists to keep the demo self-contained.  \n",
        "You can later swap these out for a database or API without changing the agent’s logic.\n",
        "\n",
        "### What’s included\n",
        "- **`medical_guidelines`** — evidence-style rules the agent can match against.\n",
        "  - Fields: `procedure`, `diagnosis`, `required_symptoms` (list), `notes` (free text).\n",
        "  - Example:\n",
        "    ```python\n",
        "    {\n",
        "      \"procedure\": \"CT Abdomen\",\n",
        "      \"diagnosis\": \"Suspected Appendicitis\",\n",
        "      \"required_symptoms\": [\"abdominal pain\", \"nausea\", \"RLQ tenderness\"],\n",
        "      \"notes\": \"CT imaging justified if appendicitis is unclear.\"\n",
        "    }\n",
        "    ```\n",
        "- **`care_recommendations`** — next-step suggestions keyed by diagnosis.\n",
        "  - Fields: `diagnosis`, `next_step`.\n",
        "  - Example:\n",
        "    ```python\n",
        "    {\n",
        "      \"diagnosis\": \"Community-Acquired Pneumonia\",\n",
        "      \"next_step\": \"Start empirical antibiotics; reserve CT for poor responders.\"\n",
        "    }\n",
        "    ```\n",
        "- **`patient_records`** — patient data including what **procedure** has been recommended to them based on their **diagnosis**.\n",
        "  - Fields: `patient_id`, `age`, `sex`, `symptoms` (list), `diagnosis`, `procedure`, `notes`.\n",
        "  - Example:\n",
        "    ```python\n",
        "    {\n",
        "      \"patient_id\": \"P101\",\n",
        "      \"age\": 38,\n",
        "      \"sex\": \"Male\",\n",
        "      \"symptoms\": [\"abdominal pain\", \"nausea\"],\n",
        "      \"diagnosis\": \"Possible early appendicitis\",\n",
        "      \"procedure\": \"CT Abdomen\",\n",
        "      \"notes\": \"Mild abdominal pain and nausea but no localized tenderness or rebound noted.\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "### How the agent uses these\n",
        "- **Guideline matching**: `procedure` + `diagnosis` → pick the closest entry in `medical_guidelines`.\n",
        "- **Validity check**: compare `patient_records[*].symptoms` vs. `required_symptoms` and read `notes`.\n",
        "- **Care plan**: map `diagnosis` → `care_recommendations[*].next_step`."
      ],
      "metadata": {
        "id": "IAvTAIQcG9yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, Literal\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.types import Command\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "medical_guidelines = [\n",
        "    {\"procedure\": \"MRI Brain\", \"diagnosis\": \"Migraine\", \"required_symptoms\": [\"headache\", \"nausea\"],\n",
        "     \"notes\": \"MRI not recommended unless neurological deficits or red flags present.\"},\n",
        "    {\"procedure\": \"CT Chest\", \"diagnosis\": \"Suspected Pulmonary Embolism\", \"required_symptoms\": [\"chest pain\", \"shortness of breath\", \"tachycardia\"],\n",
        "     \"notes\": \"CTPA appropriate for high probability PE cases with positive D-dimer.\"},\n",
        "    {\"procedure\": \"MRI Lumbar Spine\", \"diagnosis\": \"Chronic Low Back Pain\", \"required_symptoms\": [\"back pain > 6 weeks\", \"neurological deficit\"],\n",
        "     \"notes\": \"MRI only if pain persists despite conservative therapy and neuro signs are present.\"},\n",
        "    {\"procedure\": \"CT Chest\", \"diagnosis\": \"Community-Acquired Pneumonia\", \"required_symptoms\": [\"fever\", \"cough\"],\n",
        "     \"notes\": \"CT Chest reserved for inconclusive X-rays or immunocompromised patients.\"},\n",
        "    {\"procedure\": \"CT Abdomen\", \"diagnosis\": \"Suspected Appendicitis\", \"required_symptoms\": [\"abdominal pain\", \"nausea\", \"RLQ tenderness\"],\n",
        "     \"notes\": \"CT imaging justified if appendicitis is unclear.\"}\n",
        "]\n",
        "\n",
        "care_recommendations = [\n",
        "    {\"diagnosis\": \"Migraine\", \"next_step\": \"Start migraine treatment; imaging not necessary unless red flags appear.\"},\n",
        "    {\"diagnosis\": \"Suspected Pulmonary Embolism\", \"next_step\": \"Begin anticoagulation and confirm with CTPA.\"},\n",
        "    {\"diagnosis\": \"Chronic Low Back Pain\", \"next_step\": \"Refer to physiotherapy; MRI only if neuro symptoms persist.\"},\n",
        "    {\"diagnosis\": \"Community-Acquired Pneumonia\", \"next_step\": \"Start empirical antibiotics; reserve CT for poor responders.\"},\n",
        "    {\"diagnosis\": \"Suspected Appendicitis\", \"next_step\": \"Do CT to confirm and refer for surgery if positive.\"}\n",
        "]\n",
        "\n",
        "patient_records = [\n",
        "    {\"patient_id\": \"P101\", \"age\": 38, \"sex\": \"Male\", \"symptoms\": [\"abdominal pain\", \"nausea\"],\n",
        "     \"diagnosis\": \"Possible early appendicitis\", \"procedure\": \"CT Abdomen\",\n",
        "     \"notes\": \"Mild abdominal pain and nausea but no localized tenderness or rebound noted.\"},\n",
        "    {\"patient_id\": \"P102\", \"age\": 65, \"sex\": \"Female\", \"symptoms\": [\"chest pain\", \"shortness of breath\", \"tachycardia\"],\n",
        "     \"diagnosis\": \"Clinical suspicion of PE\", \"procedure\": \"CT Chest\",\n",
        "     \"notes\": \"Wells score high probability; D-dimer positive.\"},\n",
        "    {\"patient_id\": \"P103\", \"age\": 30, \"sex\": \"Female\", \"symptoms\": [\"recurrent headache\"],\n",
        "     \"diagnosis\": \"Classic migraine presentation\", \"procedure\": \"MRI Brain\",\n",
        "     \"notes\": \"No neuro signs or red flags. Typical migraine pattern.\"},\n",
        "    {\"patient_id\": \"P104\", \"age\": 45, \"sex\": \"Male\", \"symptoms\": [\"back pain > 6 weeks\", \"neurological deficit\"],\n",
        "     \"diagnosis\": \"Chronic Low Back Pain\", \"procedure\": \"MRI Lumbar Spine\",\n",
        "     \"notes\": \"Persistent low back pain with left leg numbness; unresponsive to physiotherapy.\"},\n",
        "    {\"patient_id\": \"P105\", \"age\": 70, \"sex\": \"Female\", \"symptoms\": [\"fever\", \"cough\"],\n",
        "     \"diagnosis\": \"Community-Acquired Pneumonia\", \"procedure\": \"CT Chest\",\n",
        "     \"notes\": \"Initial chest X-ray inconclusive; patient is immunocompromised with underlying COPD.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "xTiR2SR5NJfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools for the Utilization Review Agent\n",
        "\n",
        "These are **LangChain tools** (decorated with `@tool`) that the agent can call during a review.  \n",
        "They encapsulate domain logic and return **small, structured dicts** the agent can reason over.\n",
        "\n",
        "#### Summary of tools\n",
        "\n",
        "| Tool | Purpose | Inputs | Output keys |\n",
        "|---|---|---|---|\n",
        "| `fetch_patient_record` | Retrieve and summarize a patient chart from in-memory data | `patient_id: str` | `patient_summary` _(str)_, or `error` |\n",
        "| `match_guideline` | Pick the closest clinical guideline for a (procedure, diagnosis) pair using the LLM | `procedure: str`, `diagnosis: str` | `matched_guideline` _(str)_ |\n",
        "| `check_guideline_validity` | Validate whether patient symptoms/notes meet the guideline’s criteria | `symptoms: list[str]`, `required_symptoms: list[str]`, `notes: str` | `validity_result` _(str)_ |\n",
        "| `recommend_care_plan` | Suggest next steps for the given diagnosis | `diagnosis: str` | `recommendation` _(str)_ |\n",
        "\n",
        "> All LLM-backed tools use `ChatOpenAI` (temperature = 0, streaming enabled in code) and return **concise textual justifications** under a single key.\n",
        "\n",
        "#### Typical call order used by the agent\n",
        "1. `fetch_patient_record(patient_id)` → summarize context  \n",
        "2. `match_guideline(procedure, diagnosis)` → find best-fit rule  \n",
        "3. `check_guideline_validity(symptoms, required_symptoms, notes)` → approve vs. needs review  \n",
        "4. `recommend_care_plan(diagnosis)` → action steps / alternatives\n",
        "\n",
        "#### Example outputs (shape)\n",
        "```json\n",
        "// fetch_patient_record\n",
        "{ \"patient_summary\": \"Patient ID: P102\\nAge: 65, Sex: Female\\nReported Symptoms: chest pain, shortness of breath, tachycardia\\nPreliminary Diagnosis: Clinical suspicion of PE\\nRequested Procedure: CT Chest\\nClinical Notes: Wells score high probability; D-dimer positive.\" }\n",
        "\n",
        "// match_guideline\n",
        "{ \"matched_guideline\": \"CTPA is appropriate for high-probability PE with positive D-dimer. Required symptoms: chest pain, shortness of breath, tachycardia. Caveats: ensure renal function adequate for contrast.\" }\n",
        "\n",
        "// check_guideline_validity\n",
        "{ \"validity_result\": \"Criteria met: symptoms align and notes indicate high probability (Wells) with positive D-dimer. Imaging is medically necessary.\" }\n",
        "\n",
        "// recommend_care_plan\n",
        "{ \"recommendation\": \"Begin anticoagulation and confirm with CTPA; monitor hemodynamics; consider risk stratification.\" }\n"
      ],
      "metadata": {
        "id": "ZWq9Qnp8HvNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "@tool\n",
        "def fetch_patient_record(patient_id: str) -> dict:\n",
        "    \"\"\"\n",
        "    Fetches and summarizes a patient record based on the given patient ID.\n",
        "\n",
        "    Returns a human-readable summary including age, sex, symptoms, diagnosis, procedure, and clinical notes.\n",
        "    Also includes the raw patient record in case other tools or agents need structured access.\n",
        "\n",
        "    Args:\n",
        "        patient_id (str): The unique identifier for the patient.\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            \"summary\": str,  # Natural language summary of the patient record\n",
        "        }\n",
        "    \"\"\"\n",
        "    for record in patient_records:\n",
        "        if record[\"patient_id\"] == patient_id:\n",
        "            summary = (\n",
        "                f\"Patient ID: {record['patient_id']}\\n\"\n",
        "                f\"Age: {record['age']}, Sex: {record['sex']}\\n\"\n",
        "                f\"Reported Symptoms: {', '.join(record['symptoms'])}\\n\"\n",
        "                f\"Preliminary Diagnosis: {record['diagnosis']}\\n\"\n",
        "                f\"Requested Procedure: {record['procedure']}\\n\"\n",
        "                f\"Clinical Notes: {record['notes']}\"\n",
        "            )\n",
        "            return {\n",
        "                \"patient_summary\": summary\n",
        "            }\n",
        "    return {\"error\": \"Patient record not found.\"}\n",
        "\n",
        "\n",
        "@tool\n",
        "def match_guideline(procedure: str, diagnosis: str) -> dict:\n",
        "    \"\"\"\n",
        "    Match a given procedure and diagnosis to the most relevant clinical guideline.\n",
        "\n",
        "    Args:\n",
        "        procedure (str): The medical procedure being requested.\n",
        "        diagnosis (str): The diagnosis related to the procedure.\n",
        "\n",
        "    Returns:\n",
        "        dict: A summary of the best matching guideline if found, or a message indicating no match.\n",
        "    \"\"\"\n",
        "    context = \"\\n\".join([\n",
        "        f\"{i+1}. Procedure: {g['procedure']}, Diagnosis: {g['diagnosis']}, Required Symptoms: {g['required_symptoms']}, Notes: {g['notes']}\"\n",
        "        for i, g in enumerate(medical_guidelines)])\n",
        "\n",
        "    prompt = f\"\"\"You are a clinical reviewer assessing whether a requested medical procedure aligns with existing evidence-based guidelines.\n",
        "\n",
        "Instructions:\n",
        "- Analyze the patient's procedure and diagnosis.\n",
        "- Compare against the list of provided clinical guidelines.\n",
        "- Select the guideline that best fits the case by reasoning on the common matches considering procedure and diagnosis.\n",
        "- If none match, respond: \"No appropriate guideline found for this case.\"\n",
        "- If a match is found, summarize the matching guideline clearly including any required symptoms or caveats.\n",
        "\n",
        "Patient Case:\n",
        "- Procedure: {procedure}\n",
        "- Diagnosis: {diagnosis}\n",
        "\n",
        "Available Guidelines:\n",
        "{context}\n",
        "\"\"\"\n",
        "    result = llm.invoke(prompt).content\n",
        "    return {\"matched_guideline\": result}\n",
        "\n",
        "\n",
        "@tool\n",
        "def check_guideline_validity(symptoms: list, required_symptoms: list, notes: str) -> dict:\n",
        "    \"\"\"\n",
        "    Determine whether the patient's symptoms and notes satisfy the guideline criteria for medical necessity.\n",
        "\n",
        "    Args:\n",
        "        symptoms (list): List of symptoms recorded in the patient’s record.\n",
        "        required_symptoms (list): List of symptoms required by the matched guideline.\n",
        "        notes (str): Free-text clinical notes associated with the patient case.\n",
        "\n",
        "    Returns:\n",
        "        dict: A string with justification explaining whether the procedure is valid or not.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are validating a medical procedure request based on documented symptoms and clinical context.\n",
        "\n",
        "Instructions:\n",
        "- Assess whether the patient's symptoms and notes fulfill the required guideline criteria.\n",
        "- Consider nuances or indirect references (e.g. \"long flight\" implies immobility).\n",
        "- Provide a reasoned judgment if the procedure is medically necessary.\n",
        "- If it does not qualify, explain exactly which criteria are unmet.\n",
        "\n",
        "Input:\n",
        "- Patient Symptoms: {symptoms}\n",
        "- Required Symptoms from Guideline: {required_symptoms}\n",
        "- Clinical Notes: {notes}\n",
        "\"\"\"\n",
        "    result = llm.invoke(prompt).content\n",
        "    return {\"validity_result\": result}\n",
        "\n",
        "\n",
        "@tool\n",
        "def recommend_care_plan(diagnosis: str) -> dict:\n",
        "    \"\"\"\n",
        "    Recommend a follow-up care plan based on a given diagnosis.\n",
        "\n",
        "    Args:\n",
        "        diagnosis (str): The diagnosis to evaluate for next steps.\n",
        "\n",
        "    Returns:\n",
        "        dict: A recommendation string describing the suggested care plan or a fallback message if no match is found.\n",
        "    \"\"\"\n",
        "    options = \"\\n\".join([\n",
        "        f\"{i+1}. Diagnosis: {c['diagnosis']}, Recommendation: {c['next_step']}\"\n",
        "        for i, c in enumerate(care_recommendations)])\n",
        "\n",
        "    prompt = f\"\"\"You are a clinical support assistant suggesting appropriate next steps for a given medical diagnosis.\n",
        "\n",
        "Instructions:\n",
        "- Analyze the given diagnosis.\n",
        "- Choose the closest match from the list of known recommendations.\n",
        "- Explain why the match is appropriate.\n",
        "- If no suitable recommendation is found, return: \"No care recommendation found for this diagnosis.\"\n",
        "\n",
        "Diagnosis Provided:\n",
        "{diagnosis}\n",
        "\n",
        "Available Recommendations:\n",
        "{options}\n",
        "\"\"\"\n",
        "    result = llm.invoke(prompt).content\n",
        "    return {\"recommendation\": result}"
      ],
      "metadata": {
        "id": "oHzod7nUX_ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangGraph Agent Setup\n",
        "\n",
        "This section wires up a **tool-using ReAct agent** with LangGraph. The agent reads a system prompt, plans calls to our tools, and produces a strict final summary (Decision, Reasoning, Care).\n",
        "\n",
        "#### Components\n",
        "- **System Prompt** — defines the reviewer role and enforces the final **bullet-format** output.\n",
        "- **State** — a `TypedDict` with a `messages` array aggregated by `add_messages`.\n",
        "- **LLM (ChatOpenAI)** — initialized with `streaming=True` and **bound** to tools so it can call them.\n",
        "- **Graph** — We use the building `create_react_agent` to create an agent `StateGraph`\n"
      ],
      "metadata": {
        "id": "sd0lLm3TIMBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_agent_prompt = \"\"\"\n",
        "You are a senior medical review assistant responsible for evaluating healthcare procedure requests.\n",
        "\n",
        "You must call relevant tools to do the following:\n",
        "1. Retrieve the full patient record using the patient ID.\n",
        "2. Match the requested procedure and diagnosis to clinical guidelines.\n",
        "3. Validate the match by comparing the patient's symptoms and notes to the guideline's requirements.\n",
        "4. Recommend the appropriate next steps based on the diagnosis.\n",
        "5. Output a final summary based on the guidelines given below.\n",
        "\n",
        "Analyze all the results from the tool calls before making the final decision\n",
        "\n",
        "Your final response should ONLY include the following bullets in the exact format specified:\n",
        "\n",
        "- Final Decision: [APPROVED/NEEDS REVIEW]\n",
        "- Decision Reasoning: [What criteria matched or did not match]\n",
        "- Care recommendation or alternative steps: [care plan steps to take or alternative steps if it needs review]\n",
        "\n",
        "Do NOT add any other extra content in the final response\n",
        "\"\"\"\n",
        "AGENT_SYS_PROMPT = SystemMessage(content=single_agent_prompt)\n",
        "\n",
        "single_agent = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=[fetch_patient_record, match_guideline, check_guideline_validity, recommend_care_plan],\n",
        "    prompt=AGENT_SYS_PROMPT\n",
        ")"
      ],
      "metadata": {
        "id": "By6-X5gzYBg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "single_agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "OxWwaxgmnOpB",
        "outputId": "fe0ebe8d-f683-4a36-d190-ff706bce47ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x7f13b282d0d0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def call_agent_system(agent, prompt, verbose=False):\n",
        "    events = agent.stream(\n",
        "        {\"messages\": [(\"user\", prompt)]},\n",
        "        {\"recursion_limit\": 25},\n",
        "        stream_mode=\"values\"\n",
        "    )\n",
        "    for event in events:\n",
        "        if verbose:\n",
        "            event[\"messages\"][-1].pretty_print()\n",
        "\n",
        "    print('\\nFinal Response:')\n",
        "    display(Markdown(event[\"messages\"][-1].content))"
      ],
      "metadata": {
        "id": "mGi49wBGYKpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Tracing & Monitoring your Agent\n",
        "\n",
        "Enable LangSmith tracing to capture agent runs, then execute sample utilization review queries to observe how the agent interacts with tools.  \n",
        "\n",
        "- Run test prompts through the agent (e.g., patient cases) and log traces to LangSmith.  \n",
        "- Inspect recorded traces in the LangSmith UI to review inputs, tool calls, and final outputs.  \n",
        "- Use LangSmith’s **monitoring dashboards** to track latency, errors, and token usage across runs.  \n",
        "\n"
      ],
      "metadata": {
        "id": "DQsAPAbYJsx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run your Agent"
      ],
      "metadata": {
        "id": "FRruiiFgTZUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import time\n",
        "\n",
        "for patient_id in [\"P101\", \"P102\", \"P103\", \"P104\", \"P105\"]:\n",
        "    prompt = f\"Review patient {patient_id} for procedure justification.\"\n",
        "    call_agent_system(single_agent, prompt, verbose=False)\n",
        "    time.sleep(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "-P1F4bCEZATo",
        "outputId": "b1f0ba52-712e-43d6-8b13-590f2f8d76c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Final Decision: NEEDS REVIEW\n- Decision Reasoning: The patient's symptoms include abdominal pain and nausea, but they do not exhibit the required RLQ tenderness, which is necessary for the procedure to be considered medically necessary according to the guideline.\n- Care recommendation or alternative steps: Recommend monitoring the patient and considering alternative imaging or clinical evaluation before proceeding with a CT scan. If symptoms worsen or if RLQ tenderness develops, re-evaluate for possible surgical intervention."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Final Decision: APPROVED\n- Decision Reasoning: The patient's symptoms (chest pain, shortness of breath, tachycardia) match the required symptoms for the CT Chest procedure as per the guideline for suspected pulmonary embolism (PE). The clinical context (high probability Wells score and positive D-dimer) further supports the medical necessity of the procedure.\n- Care recommendation or alternative steps: Begin anticoagulation and confirm with CTPA."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Final Decision: NEEDS REVIEW\n- Decision Reasoning: The patient's symptoms include \"recurrent headache,\" which matches one required symptom, but the absence of \"nausea\" does not meet the guideline criteria for an MRI. The guideline specifies that an MRI is not warranted without neurological deficits or red flags.\n- Care recommendation or alternative steps: Start migraine treatment; imaging not necessary unless red flags appear."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Final Decision: APPROVED\n- Decision Reasoning: The patient's symptoms of back pain for more than 6 weeks and neurological deficit (left leg numbness) meet the guideline criteria for the MRI procedure, and conservative treatment has been ineffective.\n- Care recommendation or alternative steps: Proceed with the MRI of the lumbar spine; consider referral to physiotherapy for ongoing management of chronic low back pain."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Final Decision: APPROVED\n- Decision Reasoning: The patient's symptoms of fever and cough match the guideline's required symptoms. The patient is immunocompromised, and the initial chest X-ray was inconclusive, justifying the need for a CT Chest.\n- Care recommendation or alternative steps: Start empirical antibiotics for Community-Acquired Pneumonia; reserve CT for poor responders."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 962 ms, sys: 82.6 ms, total: 1.04 s\n",
            "Wall time: 1min 54s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Traces on LangSmith\n",
        "\n",
        "View the recorded traces in the LangSmith UI to analyze how the agent processed each query.  \n",
        "\n",
        "- Inspect **inputs and outputs** for every run.  \n",
        "- Review **tool calls** in sequence to understand the decision-making path.  \n",
        "- Check **timings, token usage, and costs** to evaluate efficiency.  \n",
        "- Use the trace viewer to debug unexpected behaviors or confirm correct reasoning.  \n",
        "\n",
        "\n",
        "![](https://i.imgur.com/k5XC0pU.png)"
      ],
      "metadata": {
        "id": "7jzYKg0MLUAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Agent on Patients\n",
        "\n",
        "Execute the agent on sample patient cases\n",
        "\n",
        "\n",
        "- Each query triggers full tool calls and LLM reasoning.  \n",
        "- All steps are traced to LangSmith for review.  \n"
      ],
      "metadata": {
        "id": "FO0I0SHvNgBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for patient_id in [\"P101\", \"P102\", \"P103\", \"P104\", \"P105\"]:\n",
        "    prompt = f\"Review patient {patient_id} for procedure justification.\"\n",
        "    call_agent_system(single_agent, prompt, verbose=False)\n",
        "    time.sleep(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "EPXMn6LPwnzt",
        "outputId": "bf343a0a-1fde-422a-8886-c56735756734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Final Decision: NEEDS REVIEW\n- Decision Reasoning: The patient's symptoms include abdominal pain and nausea, but they do not exhibit the required RLQ tenderness, which is necessary for the procedure to be considered medically necessary according to the guideline.\n- Care recommendation or alternative steps: Recommend monitoring the patient and considering a follow-up evaluation. If symptoms worsen or if there are signs of appendicitis, a CT scan may be warranted later, and surgical referral should be considered if appendicitis is confirmed."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Final Decision: APPROVED\n- Decision Reasoning: The patient's symptoms (chest pain, shortness of breath, tachycardia) match the required symptoms for the CT Chest procedure as per the guideline for suspected pulmonary embolism (PE). The clinical context (high Wells score and positive D-dimer) further supports the medical necessity of the procedure.\n- Care recommendation or alternative steps: Begin anticoagulation and confirm with CTPA."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Final Decision: NEEDS REVIEW\n- Decision Reasoning: The patient's symptoms include \"recurrent headache,\" which aligns with one of the required symptoms from the guideline. However, the guideline also specifies the need for \"nausea\" as a required symptom, which the patient does not report. Therefore, the procedure is not medically necessary at this time.\n- Care recommendation or alternative steps: Start migraine treatment; imaging not necessary unless red flags appear. Focus on managing symptoms and preventing future episodes."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Final Decision: APPROVED\n- Decision Reasoning: The patient's symptoms of back pain for more than 6 weeks and neurological deficit (left leg numbness) meet the guideline criteria for the MRI procedure, and conservative treatment has been ineffective.\n- Care recommendation or alternative steps: Proceed with the MRI of the lumbar spine; continue physiotherapy and monitor neurological symptoms."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Final Decision: APPROVED\n- Decision Reasoning: The patient's symptoms of fever and cough match the guideline's required symptoms. The patient is immunocompromised, and the initial chest X-ray was inconclusive, justifying the need for a CT Chest.\n- Care recommendation or alternative steps: Start empirical antibiotics for Community-Acquired Pneumonia; reserve CT for poor responders."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 924 ms, sys: 80.4 ms, total: 1 s\n",
            "Wall time: 1min 34s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check out the traces on LangSmith\n",
        "\n",
        "You will see a standard processing time of around ~15s per patient review\n",
        "\n",
        "![](https://i.imgur.com/t1Nzt5D.png)"
      ],
      "metadata": {
        "id": "y4JE7V0ONnyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitoring your Agent\n",
        "\n",
        "Use LangSmith’s dashboards to track the health and performance of your agent over time.  \n",
        "\n",
        "- Access **pre-built dashboards** to view key metrics such as latency, error rates, token usage, and costs.  \n",
        "- Create **custom dashboards** to monitor metrics tailored to your use case.  \n",
        "- Filter by project, environment, or run tags to analyze performance across different deployments.  \n",
        "- Use monitoring insights to detect regressions, optimize efficiency, and improve reliability before scaling to production.  \n",
        "\n",
        "\n",
        "![](https://i.imgur.com/VlGeRMP.png)"
      ],
      "metadata": {
        "id": "z8huVRBuaNuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate your Utilization Review AI Agent\n",
        "\n",
        "Assess the performance and reliability of the utilization review agent using LangSmith datasets and evaluators.  \n",
        "\n",
        "- **Create Evaluation Dataset**: Build a dataset of patient queries, expected outputs, and reference trajectories.  \n",
        "- **Define Reference Trajectories**: Capture the ideal sequence of tool calls for each test case.  \n",
        "- **Upload Dataset to LangSmith**: Store examples centrally to run evaluations at scale.  \n",
        "- **Goal Accuracy & Reasoning Evaluation**: Use **OpenEvals** with LLM-as-judge to compare the agent’s final outputs against reference answers.  \n",
        "- **Trajectory Accuracy Evaluation**: Use **AgentEvals** to check if the sequence of agent steps including llm calls, tool calls, responses matches the reference trajectory.  \n",
        "- **Run Evaluation in LangSmith**: Execute the evaluators against the dataset, view results in the UI, and compare across agent versions.  \n"
      ],
      "metadata": {
        "id": "zogCfSDvUBM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create an Evaluation Dataset\n",
        "\n",
        "Prepare a dataset of test cases to systematically evaluate the agent.  \n",
        "\n",
        "- Define **patient queries** representing different utilization review scenarios.  \n",
        "- Specify the **expected outputs** for each query\n",
        "- Store the dataset in a structured format so it can be uploaded and reused in LangSmith evaluations.  \n"
      ],
      "metadata": {
        "id": "aBY6W1elUOni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_DATASET = 'Agent Evaluation Dataset - DHS 2025 - Demo'"
      ],
      "metadata": {
        "id": "K0MG2fvPUNd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "# Initialize a client\n",
        "client = Client(timeout_ms=3600000)\n",
        "\n",
        "ls_dataset = client.create_dataset(\n",
        "    dataset_name=EVAL_DATASET,\n",
        "    description=\"Dataset of sample queries and responses to evaluate the performance of a healthcare utilization review agent\",\n",
        ")"
      ],
      "metadata": {
        "id": "cdt57OUNTtqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This create an empty evaluation dataset as shown below and we can start adding test records which can be used for evaluation\n",
        "\n",
        "![](https://i.imgur.com/D8GEEfh.png)"
      ],
      "metadata": {
        "id": "NB9l2vXWVAtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Test Records to the Evaluation Dataset\n",
        "\n",
        "Populate the evaluation dataset with structured test records for the agent.  \n",
        "\n",
        "- **Input prompt**: The query or patient case given to the agent.  \n",
        "- **Output response**: The human-curated or LLM-generated reference answer showing the expected decision, reasoning, and recommendation.  \n",
        "- **Output trajectory**: Initially left empty; later populated with the full sequence of tool calls, LLM invocations, and responses observed during agent execution.  \n"
      ],
      "metadata": {
        "id": "TaTAMYMaVNVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"prompt\": \"Review patient P101 for procedure justification.\",\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"response\": \"The patient is experiencing abdominal pain and nausea, which aligns with two of the three required symptoms for a CT Abdomen in suspected appendicitis cases. However, the absence of RLQ (right lower quadrant) tenderness means the criteria are not fully met. Based on the guidelines, this case needs further review before proceeding with imaging. It is best to monitor the patient and consider clinical re-evaluation or alternative diagnostic approaches before ordering a CT scan.\",\n",
        "            \"trajectory\": []\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"prompt\": \"Review patient P102 for procedure justification.\",\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"response\": \"This patient's symptoms include chest pain, shortness of breath, and tachycardia, which match the guideline criteria for a CT Chest in suspected pulmonary embolism. In addition, the clinical notes mention a high Wells score and a positive D-dimer, both of which support the need for imaging. This procedure is medically appropriate and should be approved. Treatment with anticoagulants can be started, followed by confirmation through CTPA.\",\n",
        "            \"trajectory\": []\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"prompt\": \"Review patient P103 for procedure justification.\",\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"response\": \"The patient is presenting with recurrent headaches, which partially aligns with the guideline for an MRI Brain in migraine cases. However, the guideline also requires the presence of nausea or red flags, which are not noted in the record. Since the imaging criteria are not fully met, this case needs further review. It is better to begin migraine treatment and only consider imaging if new concerning symptoms appear.\",\n",
        "            \"trajectory\": []\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"prompt\": \"Review patient P104 for procedure justification.\",\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"response\": \"The patient reports back pain lasting more than 6 weeks along with neurological symptoms. Both are required for an MRI Lumbar Spine according to the guideline. The clinical note also states that physiotherapy has not helped, which further supports the imaging request. This procedure meets all the necessary criteria and is appropriate for approval.\",\n",
        "            \"trajectory\": []\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"inputs\": {\n",
        "            \"prompt\": \"Review patient P105 for procedure justification.\",\n",
        "        },\n",
        "        \"outputs\": {\n",
        "            \"response\": \"The patient has fever and cough, which align with the core symptoms for diagnosing pneumonia. Although a CT Chest is not typically the first imaging choice, in this case the chest X-ray was inconclusive and the patient is immunocompromised. These factors make advanced imaging justified. This procedure meets the guideline exceptions and should be approved to guide treatment effectively.\",\n",
        "            \"trajectory\": []\n",
        "        }\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "ttTPAGP8pKtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Agent Execution Trajectory per Test Record\n",
        "\n",
        "Execute the agent for each test input and capture the full step-by-step trajectory to use as the reference path.  \n",
        "\n",
        "- Run the agent on the test record to log **tool calls**, **LLM messages**, inputs, and intermediate outputs.  \n",
        "- Extract the ordered **trajectory** (all steps from start to final answer) and attach it to the test record.  \n",
        "- **Curate and validate** the trajectory with domain experts; correct any mistakes before finalizing.  \n",
        "- Save the approved trajectory as the **reference** for trajectory-based evaluations.  \n",
        "- Keep runs reproducible (fixed model/version and temperature) so trajectories remain comparable.  \n"
      ],
      "metadata": {
        "id": "WbhLMWW5VtFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for record in dataset:\n",
        "    record['outputs']['trajectory'] = single_agent.invoke({\"messages\": [(\"user\", record['inputs']['prompt'])]},\n",
        "                                        {\"recursion_limit\": 25})['messages']"
      ],
      "metadata": {
        "id": "WL2slPG5saZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What does a Sample Agent Trajectory look like?\n",
        "\n",
        "It is basically all the execution steps in the agent including tool call requests, responses, agent calls, LLM call\\responses etc."
      ],
      "metadata": {
        "id": "skWvGQPJWlEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckxPAG6us89Z",
        "outputId": "1cd2bcb1-defc-4616-bf82-18cb86ef0311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'inputs': {'prompt': 'Review patient P101 for procedure justification.'},\n",
              " 'outputs': {'response': 'The patient is experiencing abdominal pain and nausea, which aligns with two of the three required symptoms for a CT Abdomen in suspected appendicitis cases. However, the absence of RLQ (right lower quadrant) tenderness means the criteria are not fully met. Based on the guidelines, this case needs further review before proceeding with imaging. It is best to monitor the patient and consider clinical re-evaluation or alternative diagnostic approaches before ordering a CT scan.',\n",
              "  'trajectory': [HumanMessage(content='Review patient P101 for procedure justification.', additional_kwargs={}, response_metadata={}, id='5d055e81-3dd2-4305-9134-a0ea01be8515'),\n",
              "   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_2fIPR8AXz3cx6s4vR9Js96J3', 'function': {'arguments': '{\"patient_id\":\"P101\"}', 'name': 'fetch_patient_record'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 558, 'total_tokens': 575, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C5ySXWMYHHvoUC2ksR7hf9qYgy9po', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--66552807-912b-404c-bcdb-ce0f08ca1d0f-0', tool_calls=[{'name': 'fetch_patient_record', 'args': {'patient_id': 'P101'}, 'id': 'call_2fIPR8AXz3cx6s4vR9Js96J3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 558, 'output_tokens': 17, 'total_tokens': 575, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "   ToolMessage(content='{\"patient_summary\": \"Patient ID: P101\\\\nAge: 38, Sex: Male\\\\nReported Symptoms: abdominal pain, nausea\\\\nPreliminary Diagnosis: Possible early appendicitis\\\\nRequested Procedure: CT Abdomen\\\\nClinical Notes: Mild abdominal pain and nausea but no localized tenderness or rebound noted.\"}', name='fetch_patient_record', id='28e5702e-5d56-42b8-aa5f-0c5d02b1f61f', tool_call_id='call_2fIPR8AXz3cx6s4vR9Js96J3'),\n",
              "   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_xPUHqN4TCZLdXmOOiUyFvMVj', 'function': {'arguments': '{\"procedure\":\"CT Abdomen\",\"diagnosis\":\"Possible early appendicitis\"}', 'name': 'match_guideline'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 646, 'total_tokens': 672, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C5ySYaS2k1w3uWj629JuGtS4axXYV', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--e639e7fd-6518-4d02-8a6d-dd434dff36d8-0', tool_calls=[{'name': 'match_guideline', 'args': {'procedure': 'CT Abdomen', 'diagnosis': 'Possible early appendicitis'}, 'id': 'call_xPUHqN4TCZLdXmOOiUyFvMVj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 646, 'output_tokens': 26, 'total_tokens': 672, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "   ToolMessage(content='{\"matched_guideline\": \"The patient\\'s procedure is a CT Abdomen, and the diagnosis is possible early appendicitis. \\\\n\\\\nUpon reviewing the available guidelines, the one that best fits this case is:\\\\n\\\\n**Guideline: CT Abdomen, Diagnosis: Suspected Appendicitis**\\\\n- **Required Symptoms:** [\\'abdominal pain\\', \\'nausea\\', \\'RLQ tenderness\\']\\\\n- **Notes:** CT imaging justified if appendicitis is unclear.\\\\n\\\\nThis guideline indicates that a CT Abdomen is appropriate for evaluating suspected appendicitis, especially when the diagnosis is uncertain and the patient presents with relevant symptoms such as abdominal pain, nausea, and right lower quadrant tenderness. \\\\n\\\\nIn this case, since the diagnosis is possible early appendicitis, the CT Abdomen aligns with the guideline, as it is justified to clarify the diagnosis.\"}', name='match_guideline', id='dfe1ce53-b48a-433d-b4f4-f6761c79aca6', tool_call_id='call_xPUHqN4TCZLdXmOOiUyFvMVj'),\n",
              "   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_z8TzjQRH629nnk8tfWqHEsYC', 'function': {'arguments': '{\"symptoms\":[\"abdominal pain\",\"nausea\"],\"required_symptoms\":[\"abdominal pain\",\"nausea\",\"RLQ tenderness\"],\"notes\":\"Mild abdominal pain and nausea but no localized tenderness or rebound noted.\"}', 'name': 'check_guideline_validity'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 861, 'total_tokens': 920, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C5ySZKArumAJSxCw0nIPGq5eLK98t', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8c741235-4b1e-49da-b366-37f5be304d97-0', tool_calls=[{'name': 'check_guideline_validity', 'args': {'symptoms': ['abdominal pain', 'nausea'], 'required_symptoms': ['abdominal pain', 'nausea', 'RLQ tenderness'], 'notes': 'Mild abdominal pain and nausea but no localized tenderness or rebound noted.'}, 'id': 'call_z8TzjQRH629nnk8tfWqHEsYC', 'type': 'tool_call'}], usage_metadata={'input_tokens': 861, 'output_tokens': 59, 'total_tokens': 920, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "   ToolMessage(content='{\"validity_result\": \"Based on the provided information, the patient\\'s symptoms include abdominal pain and nausea, which align with two of the three required symptoms from the guideline. However, the guideline specifies that RLQ (right lower quadrant) tenderness is also necessary for the procedure request to be considered medically necessary.\\\\n\\\\nIn the clinical notes, it is mentioned that there is mild abdominal pain and nausea, but importantly, there is no localized tenderness or rebound noted. This indicates that the patient does not exhibit the required RLQ tenderness, which is a critical component of the guideline criteria.\\\\n\\\\n**Reasoned Judgment:**\\\\nThe procedure request does not qualify as medically necessary because the patient does not meet all the required symptoms outlined in the guideline. Specifically, the unmet criterion is the absence of RLQ tenderness. Without this symptom, the clinical context does not support the need for the procedure.\"}', name='check_guideline_validity', id='eac3a230-6c06-40c9-91da-5c519ef3dad8', tool_call_id='call_z8TzjQRH629nnk8tfWqHEsYC'),\n",
              "   AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wlw5wnHKvjZKXEitDzEGWWNj', 'function': {'arguments': '{\"diagnosis\":\"Possible early appendicitis\"}', 'name': 'recommend_care_plan'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 1110, 'total_tokens': 1132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C5ySbWlwxaDxRtCliO4icZw1305Tu', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--45c74176-a8dd-4b7e-af7a-08ef0b0e399e-0', tool_calls=[{'name': 'recommend_care_plan', 'args': {'diagnosis': 'Possible early appendicitis'}, 'id': 'call_wlw5wnHKvjZKXEitDzEGWWNj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1110, 'output_tokens': 22, 'total_tokens': 1132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
              "   ToolMessage(content='{\"recommendation\": \"The closest match from the available recommendations for the diagnosis of \\\\\"possible early appendicitis\\\\\" is:\\\\n\\\\n**Diagnosis: Suspected Appendicitis, Recommendation: Do CT to confirm and refer for surgery if positive.**\\\\n\\\\n**Explanation:**\\\\nThis recommendation is appropriate because it directly addresses the concern of appendicitis, which is a potential diagnosis in this case. The recommendation to perform a CT scan is crucial for confirming the diagnosis of appendicitis, as imaging can help visualize the appendix and identify any inflammation or complications. If the CT scan confirms appendicitis, the next step would typically involve surgical intervention, which aligns with the recommendation to refer for surgery if the CT results are positive. Therefore, this recommendation is the most relevant and appropriate next step for managing a case of possible early appendicitis.\"}', name='recommend_care_plan', id='2aeb97d8-cb8e-4909-b73e-b4b4252ceab8', tool_call_id='call_wlw5wnHKvjZKXEitDzEGWWNj'),\n",
              "   AIMessage(content=\"- Final Decision: NEEDS REVIEW\\n- Decision Reasoning: The patient's symptoms include abdominal pain and nausea, but they do not exhibit the required RLQ tenderness, which is necessary for the procedure to be considered medically necessary according to the guideline.\\n- Care recommendation or alternative steps: Recommend monitoring the patient and considering further evaluation or alternative imaging if symptoms persist, as the CT Abdomen is not justified at this time.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 1314, 'total_tokens': 1398, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1024}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-C5yScJUkSNarfRjirS8DViWQNhEvQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--afa529c9-cabc-4785-975b-04a151c9e2b9-0', usage_metadata={'input_tokens': 1314, 'output_tokens': 84, 'total_tokens': 1398, 'input_token_details': {'audio': 0, 'cache_read': 1024}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1dSyjcjlFoZpYEqv4P9Oi0-kU2gIoolMB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qtg12FE5W7s5",
        "outputId": "41f3630d-302d-4989-9893-4965b3b375fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1dSyjcjlFoZpYEqv4P9Oi0-kU2gIoolMB\n",
            "From (redirected): https://drive.google.com/uc?id=1dSyjcjlFoZpYEqv4P9Oi0-kU2gIoolMB&confirm=t&uuid=e92c28c1-46db-43dc-b8b6-55346e668c17\n",
            "To: /content/agent_utils.py\n",
            "\r  0% 0.00/6.38k [00:00<?, ?B/s]\r100% 6.38k/6.38k [00:00<00:00, 21.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from agent_utils import format_message"
      ],
      "metadata": {
        "id": "Yzek6-_HW06F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for event in dataset[0]['outputs']['trajectory']:\n",
        "    format_message(event)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2xqrvxNXByU",
        "outputId": "cbc16974-ce43-433b-f37b-34f244b88e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╭────────────────────────────────── 🧑 Human ──────────────────────────────────╮\n",
            "│ Review patient P101 for procedure justification.                             │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "╭─────────────────────────────── ⚙️ Tool Request ───────────────────────────────╮\n",
            "│                                                                              │\n",
            "│ 🔧 Tool Call: fetch_patient_record                                           │\n",
            "│    Args: {                                                                   │\n",
            "│   \"patient_id\": \"P101\"                                                       │\n",
            "│ }                                                                            │\n",
            "│    ID: call_2fIPR8AXz3cx6s4vR9Js96J3                                         │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "╭─────────────────────────────── 🔧 Tool Output ───────────────────────────────╮\n",
            "│ {\"patient_summary\": \"Patient ID: P101\\nAge: 38, Sex: Male\\nReported          │\n",
            "│ Symptoms: abdominal pain, nausea\\nPreliminary Diagnosis: Possible early      │\n",
            "│ appendicitis\\nRequested Procedure: CT Abdomen\\nClinical Notes: Mild          │\n",
            "│ abdominal pain and nausea but no localized tenderness or rebound noted.\"}    │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "╭─────────────────────────────── ⚙️ Tool Request ───────────────────────────────╮\n",
            "│                                                                              │\n",
            "│ 🔧 Tool Call: match_guideline                                                │\n",
            "│    Args: {                                                                   │\n",
            "│   \"procedure\": \"CT Abdomen\",                                                 │\n",
            "│   \"diagnosis\": \"Possible early appendicitis\"                                 │\n",
            "│ }                                                                            │\n",
            "│    ID: call_xPUHqN4TCZLdXmOOiUyFvMVj                                         │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "╭─────────────────────────────── 🔧 Tool Output ───────────────────────────────╮\n",
            "│ {\"matched_guideline\": \"The patient's procedure is a CT Abdomen, and the      │\n",
            "│ diagnosis is possible early appendicitis. \\n\\nUpon reviewing the available   │\n",
            "│ guidelines, the one that best fits this case is:\\n\\n**Guideline: CT Abdomen, │\n",
            "│ Diagnosis: Suspected Appendicitis**\\n- **Required Symptoms:** ['abdominal    │\n",
            "│ pain', 'nausea', 'RLQ tenderness']\\n- **Notes:** CT imaging justified if     │\n",
            "│ appendicitis is unclear.\\n\\nThis guideline indicates that a CT Abdomen is    │\n",
            "│ appropriate for evaluating suspected appendicitis, especially when the       │\n",
            "│ diagnosis is uncertain and the patient presents with relevant symptoms such  │\n",
            "│ as abdominal pain, nausea, and right lower quadrant tenderness. \\n\\nIn this  │\n",
            "│ case, since the diagnosis is possible early appendicitis, the CT Abdomen     │\n",
            "│ aligns with the guideline, as it is justified to clarify the diagnosis.\"}    │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "╭─────────────────────────────── ⚙️ Tool Request ───────────────────────────────╮\n",
            "│                                                                              │\n",
            "│ 🔧 Tool Call: check_guideline_validity                                       │\n",
            "│    Args: {                                                                   │\n",
            "│   \"symptoms\": [                                                              │\n",
            "│     \"abdominal pain\",                                                        │\n",
            "│     \"nausea\"                                                                 │\n",
            "│   ],                                                                         │\n",
            "│   \"required_symptoms\": [                                                     │\n",
            "│     \"abdominal pain\",                                                        │\n",
            "│     \"nausea\",                                                                │\n",
            "│     \"RLQ tenderness\"                                                         │\n",
            "│   ],                                                                         │\n",
            "│   \"notes\": \"Mild abdominal pain and nausea but no localized tenderness or    │\n",
            "│ rebound noted.\"                                                              │\n",
            "│ }                                                                            │\n",
            "│    ID: call_z8TzjQRH629nnk8tfWqHEsYC                                         │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "╭─────────────────────────────── 🔧 Tool Output ───────────────────────────────╮\n",
            "│ {\"validity_result\": \"Based on the provided information, the patient's        │\n",
            "│ symptoms include abdominal pain and nausea, which align with two of the      │\n",
            "│ three required symptoms from the guideline. However, the guideline specifies │\n",
            "│ that RLQ (right lower quadrant) tenderness is also necessary for the         │\n",
            "│ procedure request to be considered medically necessary.\\n\\nIn the clinical   │\n",
            "│ notes, it is mentioned that there is mild abdominal pain and nausea, but     │\n",
            "│ importantly, there is no localized tenderness or rebound noted. This         │\n",
            "│ indicates that the patient does not exhibit the required RLQ tenderness,     │\n",
            "│ which is a critical component of the guideline criteria.\\n\\n**Reasoned       │\n",
            "│ Judgment:**\\nThe procedure request does not qualify as medically necessary   │\n",
            "│ because the patient does not meet all the required symptoms outlined in the  │\n",
            "│ guideline. Specifically, the unmet criterion is the absence of RLQ           │\n",
            "│ tenderness. Without this symptom, the clinical context does not support the  │\n",
            "│ need for the procedure.\"}                                                    │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "╭─────────────────────────────── ⚙️ Tool Request ───────────────────────────────╮\n",
            "│                                                                              │\n",
            "│ 🔧 Tool Call: recommend_care_plan                                            │\n",
            "│    Args: {                                                                   │\n",
            "│   \"diagnosis\": \"Possible early appendicitis\"                                 │\n",
            "│ }                                                                            │\n",
            "│    ID: call_wlw5wnHKvjZKXEitDzEGWWNj                                         │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "╭─────────────────────────────── 🔧 Tool Output ───────────────────────────────╮\n",
            "│ {\"recommendation\": \"The closest match from the available recommendations for │\n",
            "│ the diagnosis of \\\"possible early appendicitis\\\" is:\\n\\n**Diagnosis:         │\n",
            "│ Suspected Appendicitis, Recommendation: Do CT to confirm and refer for       │\n",
            "│ surgery if positive.**\\n\\n**Explanation:**\\nThis recommendation is           │\n",
            "│ appropriate because it directly addresses the concern of appendicitis, which │\n",
            "│ is a potential diagnosis in this case. The recommendation to perform a CT    │\n",
            "│ scan is crucial for confirming the diagnosis of appendicitis, as imaging can │\n",
            "│ help visualize the appendix and identify any inflammation or complications.  │\n",
            "│ If the CT scan confirms appendicitis, the next step would typically involve  │\n",
            "│ surgical intervention, which aligns with the recommendation to refer for     │\n",
            "│ surgery if the CT results are positive. Therefore, this recommendation is    │\n",
            "│ the most relevant and appropriate next step for managing a case of possible  │\n",
            "│ early appendicitis.\"}                                                        │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "╭──────────────────────────────── 🤖 Assistant ────────────────────────────────╮\n",
            "│ - Final Decision: NEEDS REVIEW                                               │\n",
            "│ - Decision Reasoning: The patient's symptoms include abdominal pain and      │\n",
            "│ nausea, but they do not exhibit the required RLQ tenderness, which is        │\n",
            "│ necessary for the procedure to be considered medically necessary according   │\n",
            "│ to the guideline.                                                            │\n",
            "│ - Care recommendation or alternative steps: Recommend monitoring the patient │\n",
            "│ and considering further evaluation or alternative imaging if symptoms        │\n",
            "│ persist, as the CT Abdomen is not justified at this time.                    │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload Evaluation Dataset Records to LangSmith\n",
        "\n",
        "Send the curated evaluation dataset to LangSmith so it can be reused for automated testing and benchmarking.  \n",
        "\n",
        "- Use the LangSmith **Client API** to create examples from the test records.  \n",
        "- Each record includes the **input prompt**, **expected output response**, and the **reference trajectory**.  \n",
        "- Store the dataset under a clear name for easy lookup and reuse across evaluations.  \n",
        "- Uploaded datasets appear in the LangSmith UI and can be versioned or shared with team members.  \n"
      ],
      "metadata": {
        "id": "rPZNDL-8XL0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client(timeout_ms=3600000)\n",
        "\n",
        "client.create_examples(\n",
        "        dataset_id=ls_dataset.id,\n",
        "        examples=dataset\n",
        ")"
      ],
      "metadata": {
        "id": "QcMhAMvD5mX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now go to LangSmith and see inside the empty dataset, you will have 5 new records which can be used to test and evaluate your agent\n",
        "\n",
        "![](https://i.imgur.com/XzAjhWm.png)"
      ],
      "metadata": {
        "id": "OoZE1PaWYgBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create LLM-as-a-Judge Agent Evaluators\n",
        "\n",
        "Define three rubric-driven evaluators to score the agent’s final answer and its internal trajectory.\n",
        "\n",
        "- **Goal Accuracy (`goal_accuracy`)**  \n",
        "  Compares the agent’s **final decision** (APPROVED / NEEDS REVIEW) against the **human reference answer**.  \n",
        "  Uses `create_llm_as_judge` with a clinical rubric; evaluates agreement on the core decision, ignoring phrasing/formatting.\n",
        "\n",
        "- **Goal Reasoning Quality (`goal_reasoning_quality`)**  \n",
        "  Judges the **quality of reasoning** supporting the final decision relative to the reference.  \n",
        "  Uses `create_llm_as_judge` to assess whether the explanation is sound, clinically grounded, and aligned with the reference rationale.\n",
        "\n",
        "- **Trajectory Accuracy (`trajectory_accuracy`)**  \n",
        "  Evaluates the **step-by-step trajectory** (tool calls + intermediate reasoning) against a **reference trajectory**.  \n",
        "  Uses `create_trajectory_llm_as_judge` with a rubric for logical progression, medical appropriateness, and efficient tool use; allows minor wording/order variations when semantically equivalent.\n",
        "\n",
        "**Implementation notes**\n",
        "- All evaluators are initialized with **`model=\"openai:gpt-4o-mini\"`**, `use_reasoning=True`, and distinct `feedback_key`s.  \n",
        "- `run_agent()` wraps the LangGraph call and returns `{\"response\": result[\"messages\"]}` for downstream evaluators.  \n",
        "- Inputs to evaluators:  \n",
        "  - `inputs['prompt']` → the test query  \n",
        "  - `outputs['response'][-1].content` → agent’s final answer (for goal-based evaluators)  \n",
        "  - `reference_outputs['response']` → reference final answer  \n",
        "  - `reference_outputs['trajectory']` → reference step sequence (for trajectory evaluator)\n"
      ],
      "metadata": {
        "id": "f4zj9bKwX4Yq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openevals.llm import create_llm_as_judge\n",
        "from agentevals.trajectory.llm import create_trajectory_llm_as_judge\n",
        "\n",
        "\n",
        "def run_agent(inputs):\n",
        "    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\n",
        "    result = single_agent.invoke({\"messages\": [{ \"role\": \"user\", \"content\": inputs['prompt']}]},\n",
        "                                 config={\"env\": \"test001\"})\n",
        "    return {\"response\": result[\"messages\"]}\n",
        "\n",
        "def goal_accuracy(inputs: dict, outputs: dict, reference_outputs: dict):\n",
        "\n",
        "    GOAL_ACCURACY_SYS_PROMPT = \"\"\"You are a clinical evaluation expert. Your task is to compare the decision made by an AI medical review assistant against a trusted human reference answer.\n",
        "\n",
        "    You will be given:\n",
        "    1. **AI Agent's Final Response**\n",
        "    2. **Human Reference Answer**\n",
        "\n",
        "    Focus only on whether the **Final Decision** made by the AI (either \"APPROVED\" or \"NEEDS REVIEW\") is medically correct based on the explanation provided in the **Human Reference Answer**.\n",
        "\n",
        "    Do not penalize the AI for minor differences in phrasing. Instead, verify if the AI's final decision **agrees or disagrees** with the core clinical judgment in the reference.\n",
        "\n",
        "    Ignore formatting differences, and do not rephrase.\n",
        "\n",
        "\n",
        "    <input>\n",
        "    {inputs}\n",
        "    </input>\n",
        "\n",
        "    <output>\n",
        "    {outputs}\n",
        "    </output>\n",
        "\n",
        "    Use the reference outputs below to help you evaluate the correctness of the response:\n",
        "\n",
        "    <reference_outputs>\n",
        "    {reference_outputs}\n",
        "    </reference_outputs>\n",
        "    \"\"\"\n",
        "\n",
        "    evaluator = create_llm_as_judge(\n",
        "        prompt=GOAL_ACCURACY_SYS_PROMPT,\n",
        "        model=\"openai:gpt-4o-mini\",\n",
        "        feedback_key=\"Goal-Accuracy\",\n",
        "        continuous=False,\n",
        "        use_reasoning=True,\n",
        "    )\n",
        "    eval_result = evaluator(\n",
        "        inputs=inputs['prompt'],\n",
        "        outputs=outputs['response'][-1].content,\n",
        "        reference_outputs=reference_outputs['response']\n",
        "    )\n",
        "    return eval_result\n",
        "\n",
        "\n",
        "def goal_reasoning_quality(inputs: dict, outputs: dict, reference_outputs: dict):\n",
        "\n",
        "    GOAL_REASONING_QUALITY_SYS_PROMPT = \"\"\"You are a clinical evaluation expert. Your task is to compare the decision made by an AI medical review assistant against a trusted human reference answer.\n",
        "\n",
        "    You will be given:\n",
        "    1. **AI Agent's Final Response**\n",
        "    2. **Human Reference Answer**\n",
        "\n",
        "    Focus only on whether the **Final Decision** made by the AI (either \"APPROVED\" or \"NEEDS REVIEW\") is medically correct based on the explanation provided in the **Human Reference Answer**.\n",
        "\n",
        "    Do not penalize the AI for minor differences in phrasing. Instead, verify if the AI's final decision **agrees or disagrees** with the core clinical judgment in the reference.\n",
        "\n",
        "    Ignore formatting differences, and do not rephrase.\n",
        "\n",
        "\n",
        "    <input>\n",
        "    {inputs}\n",
        "    </input>\n",
        "\n",
        "    <output>\n",
        "    {outputs}\n",
        "    </output>\n",
        "\n",
        "    Use the reference outputs below to help you evaluate the correctness of the response:\n",
        "\n",
        "    <reference_outputs>\n",
        "    {reference_outputs}\n",
        "    </reference_outputs>\n",
        "    \"\"\"\n",
        "\n",
        "    evaluator = create_llm_as_judge(\n",
        "        prompt=GOAL_REASONING_QUALITY_SYS_PROMPT,\n",
        "        model=\"openai:gpt-4o-mini\",\n",
        "        feedback_key=\"Goal-Good-Reasoning-Quality\",\n",
        "        continuous=False,\n",
        "        use_reasoning=True,\n",
        "\n",
        "    )\n",
        "    eval_result = evaluator(\n",
        "        inputs=inputs['prompt'],\n",
        "        outputs=outputs['response'][-1].content,\n",
        "        reference_outputs=reference_outputs['response']\n",
        "    )\n",
        "    return eval_result\n",
        "\n",
        "\n",
        "\n",
        "def trajectory_accuracy(inputs: dict, outputs: dict, reference_outputs: dict):\n",
        "\n",
        "    TRAJECTORY_ACCURACY_SYS_PROMPT = \"\"\"You are a clinical reasoning evaluator with expertise in reviewing multi-step medical AI workflows.\n",
        "\n",
        "    Your task is to assess the accuracy of an AI medical review agent's internal trajectory — the step-by-step reasoning and tool outputs used to arrive at its final decision.\n",
        "\n",
        "    <Rubric>\n",
        "    An accurate trajectory:\n",
        "    - Makes logical and clinical sense between each step\n",
        "    - Demonstrates clear and medically appropriate progression of thought\n",
        "    - Uses tools or steps efficiently and avoids redundant or irrelevant actions\n",
        "    - Is semantically equivalent to the provided reference trajectory, even if phrasing or order slightly differs\n",
        "    </Rubric>\n",
        "\n",
        "    Use the rubric to compare the actual trajectory against the trusted reference trajectory below.\n",
        "\n",
        "    <reference_trajectory>\n",
        "    {reference_outputs}\n",
        "    </reference_trajectory>\n",
        "\n",
        "    Grade this actual trajectory:\n",
        "\n",
        "    <trajectory>\n",
        "    {outputs}\n",
        "    </trajectory>\n",
        "    \"\"\"\n",
        "\n",
        "    evaluator = create_trajectory_llm_as_judge(\n",
        "        prompt=TRAJECTORY_ACCURACY_SYS_PROMPT,\n",
        "        model=\"openai:gpt-4o-mini\",\n",
        "        feedback_key=\"Trajectory-Accuracy\",\n",
        "        continuous=False,\n",
        "        use_reasoning=True,\n",
        "\n",
        "    )\n",
        "    eval_result = evaluator(\n",
        "        inputs=inputs['prompt'],\n",
        "        outputs=outputs['response'], # send full agent step by step execution trajectory\n",
        "        reference_outputs=reference_outputs['trajectory']\n",
        "    )\n",
        "    return eval_result\n",
        "\n"
      ],
      "metadata": {
        "id": "lRpw-arf1aKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate your Utilization Review Agent on the Test Dataset with your Custom Evaluators\n",
        "\n",
        "Run batch evaluations in LangSmith using the curated dataset and the three LLM-as-judge evaluators.  \n",
        "\n",
        "- Initialize the **LangSmith Client** and load the dataset by name.  \n",
        "- Provide a runner (`run_agent`) that invokes the LangGraph agent for each test case.  \n",
        "- Attach the custom evaluators: **Goal Accuracy**, **Goal Reasoning Quality**, and **Trajectory Accuracy**.  \n",
        "- Call `client.evaluate(...)` to execute the suite over all examples and record results.  \n",
        "- Review pass/fail labels, comments, and scores in the LangSmith UI; compare runs across model/prompt versions.  \n",
        "- Use results to spot regressions, refine prompts/tools, and rerun until metrics are stable.  \n"
      ],
      "metadata": {
        "id": "tSYhEgyGYqnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client(timeout_ms=3600000)\n",
        "\n",
        "experiment_results = client.evaluate(\n",
        "    run_agent,\n",
        "    data=ls_dataset.name,\n",
        "    evaluators=[goal_accuracy,\n",
        "                goal_reasoning_quality,\n",
        "                trajectory_accuracy],\n",
        "    experiment_prefix=\"agent-evals-dhs-\",\n",
        "    num_repetitions=1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "97677de96f1d47adae6806237d3eaadb",
            "0c599791678946e38680b6ec21a1a174",
            "c37f356501b44f2882b64734caf3035b",
            "d52718151c0e4b5fa00b2658f80d69ca",
            "94a926a918d3494e969538d694ece805",
            "225713878d7145a3aae647104eba23a1",
            "e0b4d4670a624088b02624ebb16f3299",
            "7922afe2a4584d44bd16ca1802a1f366",
            "a0d14701b8eb4af7ac1dc63cc6e9ed48",
            "c1634bc906fe4bc389c65469bf8038e7",
            "105be6b2b1f04882b32c0071b0688b02"
          ]
        },
        "id": "S2-aqMjv4O5v",
        "outputId": "ae49bc9e-65ee-4344-99f2-6824373eb19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'agent-evals-dhs--b8b72118' at:\n",
            "https://smith.langchain.com/o/a0ce8312-2910-55c9-8948-b4d123aa5d69/datasets/d711b90a-0264-447d-948a-8ba8d1ab0824/compare?selectedSessions=cd3e280a-1eaf-4242-a97d-bbb2d247d486\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97677de96f1d47adae6806237d3eaadb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then go to LangSmith and check out the evaluation results as depicted below in a default dashboard view.\n",
        "\n",
        "![](https://i.imgur.com/vVsyVKN.png)\n",
        "\n",
        "You can also click on any eval experiment run and drill down into detailed output traces for any experiment as depicted below.\n",
        "\n",
        "![](https://i.imgur.com/yw5bd5T.png)"
      ],
      "metadata": {
        "id": "cK34-tq8ZSM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Custom Evaluators\n",
        "\n",
        "This is only if you want to create your own evaluators without using any libraries"
      ],
      "metadata": {
        "id": "V3mKLRw1iY4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create your own custom Evaluator functions with custom schema if needed\n",
        "# def goal_accuracy(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
        "#     \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\"\n",
        "\n",
        "#     GOAL_ACCURACY_SYS_PROMPT = \"\"\"You are a clinical evaluation expert. Your task is to compare the decision made by an AI medical review assistant against a trusted human reference answer.\n",
        "\n",
        "#     You will be given:\n",
        "#     1. **AI Agent's Final Response**\n",
        "#     2. **Human Reference Answer**\n",
        "\n",
        "#     Focus only on whether the **Final Decision** made by the AI (either \"APPROVED\" or \"NEEDS REVIEW\") is medically correct based on the explanation provided in the **Human Reference Answer**.\n",
        "\n",
        "#     Do not penalize the AI for minor differences in phrasing. Instead, verify if the AI's final decision **agrees or disagrees** with the core clinical judgment in the reference.\n",
        "\n",
        "#     Ignore formatting differences, and do not rephrase. Just return the evaluation in the following JSON format (no markdown):\n",
        "\n",
        "\n",
        "#     {{\n",
        "#      \"correct_decision\": true or false,\n",
        "#      \"explanation\": \"Concise explanation of whether the AI's decision matches or contradicts the human reference, and why\"\n",
        "#     }}\n",
        "#     \"\"\"\n",
        "\n",
        "#     user = f\"\"\"INPUT PROMPT: {inputs['prompt']}\n",
        "#                HUMAN REFERENCE RESPONSE: {reference_outputs['response']}\n",
        "#                AI AGENT RESPONSE: {outputs['response'][-1].content}\"\"\"\n",
        "\n",
        "#     result = llm.invoke([{\"role\": \"system\", \"content\": GOAL_ACCURACY_SYS_PROMPT},\n",
        "#                                {\"role\": \"user\", \"content\": user}]).content\n",
        "\n",
        "#     if result.startswith(\"```json\"):\n",
        "#         result = result.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "#     result = json.loads(result)\n",
        "#     return {\n",
        "#         \"key\": \"goal_accuracy\",\n",
        "#         \"score\": result[\"correct_decision\"],\n",
        "#         \"comment\": result[\"explanation\"]\n",
        "#     }\n",
        "\n",
        "\n",
        "# def goal_reasoning_quality(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
        "#     \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\"\n",
        "\n",
        "#     GOAL_REASONING_QUALITY_SYS_PROMPT = \"\"\"You are a clinical evaluation expert. Your task is to assess the quality of the **Decision Reasoning** provided by an AI medical review assistant by comparing it to a trusted human reference answer.\n",
        "\n",
        "#     You will be given:\n",
        "#     1. **AI Agent's Final Response**\n",
        "#     2. **Human Reference Answer**\n",
        "\n",
        "#     Focus only on the **\"Decision Reasoning\"** part of the AI response. Evaluate whether the reasoning:\n",
        "#     - Is clinically sound\n",
        "#     - Accurately reflects key symptoms or criteria from the guideline\n",
        "#     - Aligns with the explanation and logic in the human reference\n",
        "#     - Is neither vague nor misleading\n",
        "\n",
        "#     Do not evaluate the Final Decision itself. You are only evaluating how well the AI justified its decision.\n",
        "\n",
        "#     Do not penalize for minor wording differences. Focus on clinical accuracy and alignment of the reasoning.\n",
        "\n",
        "#     Return your evaluation in the following strict JSON format (no markdown):\n",
        "\n",
        "#     {{\n",
        "#      \"good_reasoning\": true or false,\n",
        "#      \"explanation\": \"Concise explanation of whether the AI's reasoning was medically accurate and aligned with the reference, or why it fell short\"\n",
        "#     }}\n",
        "#     \"\"\"\n",
        "\n",
        "#     user = f\"\"\"INPUT PROMPT: {inputs['prompt']}\n",
        "#                HUMAN REFERENCE RESPONSE: {reference_outputs['response']}\n",
        "#                AI AGENT RESPONSE: {outputs['response'][-1].content}\"\"\"\n",
        "\n",
        "#     result = llm.invoke([{\"role\": \"system\", \"content\": GOAL_REASONING_QUALITY_SYS_PROMPT},\n",
        "#                                {\"role\": \"user\", \"content\": user}]).content\n",
        "\n",
        "#     if result.startswith(\"```json\"):\n",
        "#         result = result.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "#     result = json.loads(result)\n",
        "#     return {\n",
        "#         \"key\": \"goal_good_reasoning_quality\",\n",
        "#         \"score\": result[\"good_reasoning\"],\n",
        "#         \"comment\": result[\"explanation\"]\n",
        "#     }\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-wHDE_Bf1Vf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}