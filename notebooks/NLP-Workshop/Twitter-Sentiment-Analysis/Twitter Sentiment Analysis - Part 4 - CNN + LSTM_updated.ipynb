{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf0f07c-dfec-41ee-9b8f-34d9495c27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00f4fa4-6a75-4dd5-b248-3d4c48cc44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "tweets = pd.read_pickle(\"cleaned_tweets_v1.pkl\")\n",
    "X = tweets['cleaned_tweets']\n",
    "y = tweets['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09638656-7814-4f53-aefd-35d03a552888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweets</th>\n",
       "      <th>cleaned_tweets_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "      <td>fingerprint pregnancy test android apps beauti...</td>\n",
       "      <td>fingerprint pregnancy test android apps beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "      <td>finally transparant silicon case thanks uncle ...</td>\n",
       "      <td>finally transparant silicon case thanks uncle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "      <td>love this would you talk makememories unplug r...</td>\n",
       "      <td>love talk makememories unplug relax iphone sma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "      <td>wired know george wa made that way iphone cute...</td>\n",
       "      <td>wired know george way iphone cute daventry home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "      <td>what amazing service apple will not even talk ...</td>\n",
       "      <td>amazing service apple talk question unless pay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet  \\\n",
       "0      1  #fingerprint #Pregnancy Test https://goo.gl/h1...   \n",
       "1      1  Finally a transparant silicon case ^^ Thanks t...   \n",
       "2      1  We love this! Would you go? #talk #makememorie...   \n",
       "3      1  I'm wired I know I'm George I was made that wa...   \n",
       "4      0  What amazing service! Apple won't even talk to...   \n",
       "\n",
       "                                      cleaned_tweets  \\\n",
       "0  fingerprint pregnancy test android apps beauti...   \n",
       "1  finally transparant silicon case thanks uncle ...   \n",
       "2  love this would you talk makememories unplug r...   \n",
       "3  wired know george wa made that way iphone cute...   \n",
       "4  what amazing service apple will not even talk ...   \n",
       "\n",
       "                    cleaned_tweets_without_stopwords  \n",
       "0  fingerprint pregnancy test android apps beauti...  \n",
       "1  finally transparant silicon case thanks uncle ...  \n",
       "2  love talk makememories unplug relax iphone sma...  \n",
       "3    wired know george way iphone cute daventry home  \n",
       "4  amazing service apple talk question unless pay...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1889653-ca84-4bf3-a7de-0ac1a2d63cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02113e92-8480-4c45-8c97-f2460abaa7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0080e8f9-3b13-4fcd-9ee6-933154cd417c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6336,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f50cb0-7c99-49ce-8cb7-c13075d4144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.74416\n",
       "0    0.25584\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b99f490a-6b93-4192-acb0-46cff27912b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    0.744318\n",
       "0    0.255682\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad59bc4a-1580-4fb7-835f-53fded0bd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary from the trianing set only\n",
    "counter = Counter()\n",
    "for tweet in X_train:\n",
    "    counter.update(tweet.split())\n",
    "\n",
    "vocab = set(counter.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f7c6dfe-7d8f-4325-b351-f3b502da4d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d972816-2115-489e-8253-082a4c6a808c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da8d1cc0-960b-402c-9776-05d290b53959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13491\n",
      "Top 10 tokens: [('iphone', 3354), ('apple', 2326), ('the', 1601), ('samsung', 1147), ('and', 1016), ('new', 947), ('you', 912), ('twitter', 898), ('for', 855), ('phone', 851)]\n"
     ]
    }
   ],
   "source": [
    "# Save vocabulary\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for word in vocab:\n",
    "        f.write(f\"{word}\\n\")\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"Top 10 tokens:\", counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a2021e8-879a-4336-b0a3-da2ad84dc0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum length of tweets\n",
    "max_length = max(len(tweet.split()) for tweet in X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d6684ab-7b91-4d6c-ad28-9db9410c11e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60c0b013-b691-4b0f-9273-b34520cda63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bamboo', 'nancy', 'apocalyptic', 'bolly', 'feed', 'wtofg', 'makan', 'fucktheiphone', 'tqyuca', 'neverland']\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3e11822-cfa5-4302-a448-7563555d989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word to index mapping\n",
    "word_to_idx = {word: i+1 for i, word in enumerate(vocab)}  # dict comprehension\n",
    "word_to_idx['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39dd2025-20e1-4969-9a3e-5a3af347d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a01fda-e9be-4ed8-8c17-14ba86182916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5880b479-b56c-40e5-9af5-b8e2e810b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tweets to sequences of indices\n",
    "def text_to_sequence(text):\n",
    "    return [word_to_idx.get(word, 0) for word in text.split()]\n",
    "\n",
    "X_train_seq = [text_to_sequence(tweet) for tweet in X_train]\n",
    "X_test_seq = [text_to_sequence(tweet) for tweet in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193e679f-d645-46a8-8d7e-dbd2fa785bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       " 'fucking done with the apple iphone never fucking again lost everything fuck you apple die apple')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.iloc[0].split()), X_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb4f01e-75dd-45ef-b351-d41904d8540a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "822bcefc-89bf-4fa3-a7a9-158861ac56be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12894, 5188, 2413, 3583, 3190, 9277, 5914, 12894, 7487, 2372, 46, 7887, 3401, 3190, 3517, 3190]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86ce993c-0e18-451d-a461-2bbb15889284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3041"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx[\"wash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0a082b2-0a5c-4efd-9aab-cdf6e1acf454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3041"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx[\"wash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be4dd1f6-eb17-4d79-90de-d03b4e1401cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 6, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_seq[0]), len(X_train_seq[1]), len(X_train_seq[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "729c1e62-5d99-4eda-971f-b14aaa502a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "def pad_sequences(sequences, maxlen):\n",
    "    return [seq + [0]*(maxlen - len(seq)) if len(seq) < maxlen else seq[:maxlen] for seq in sequences]\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, max_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6e76019-83bc-40d4-b846-f9f927ef69e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12894, 5188, 2413, 3583, 3190, 9277, 5914, 12894, 7487, 2372, 46, 7887, 3401, 3190, 3517, 3190, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f11cc1df-6a9f-4cba-bf61-627b223c927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3070, 2505, 9337, 5769, 7768, 9277, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_pad[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35751c70-0cd6-41c5-b879-12ae900c644e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29a9b5cb-e295-4305-b7c7-4abb6b2c2a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.LongTensor(X_train_pad)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values)\n",
    "X_test_tensor = torch.LongTensor(X_test_pad)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d26fc4bc-be02-4bda-b040-662108c5f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TweetDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TweetDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f63aea3-be04-45c1-97be-1485c87c2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, kernel_size, max_length):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(num_filters * ((max_length - kernel_size + 1)//2), 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x).permute(0, 2, 1)\n",
    "        conv_out = torch.relu(self.conv1(embedded))\n",
    "        pooled = self.pool(conv_out)\n",
    "        flattened = pooled.view(pooled.size(0), -1)\n",
    "        fc1_out = torch.relu(self.fc1(flattened))\n",
    "        dropped = self.dropout(fc1_out)\n",
    "        fc2_out = self.fc2(dropped)\n",
    "        return torch.sigmoid(fc2_out).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec404037-5ae2-409e-91c5-bd0af33ebcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<PAD>\" in list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37a1bbc7-93d6-44b1-ace4-a19fb8a0cd5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"lte\" in list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc008656-b4e2-46cc-af26-bd8a36768efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bamboo', 'survive')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab)[0], list(vocab)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ba42221-031e-482f-bab9-2775b15d723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 50\n",
    "num_filters = 32\n",
    "kernel_size = 4\n",
    "vocab_size = len(vocab) + 1  # +1 for padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f75b042d-9412-498d-a640-0d84edf67efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5711, Accuracy: 74.38%\n",
      "Epoch [2/10], Loss: 0.4836, Accuracy: 74.89%\n",
      "Epoch [3/10], Loss: 0.3984, Accuracy: 79.14%\n",
      "Epoch [4/10], Loss: 0.3482, Accuracy: 81.09%\n",
      "Epoch [5/10], Loss: 0.3197, Accuracy: 81.44%\n",
      "Epoch [6/10], Loss: 0.2819, Accuracy: 86.38%\n",
      "Epoch [7/10], Loss: 0.2593, Accuracy: 87.55%\n",
      "Epoch [8/10], Loss: 0.2268, Accuracy: 88.90%\n",
      "Epoch [9/10], Loss: 0.2065, Accuracy: 89.87%\n",
      "Epoch [10/10], Loss: 0.1874, Accuracy: 90.15%\n",
      "Test Loss: 0.3654, Test Accuracy: 85.35%\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn_model = CNNModel(vocab_size, embedding_dim, num_filters, kernel_size, max_length).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        epoch_acc += (preds == labels).sum().item()\n",
    "    \n",
    "    epoch_loss /= len(train_dataset)\n",
    "    epoch_acc /= len(train_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluation\n",
    "cnn_model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        test_acc += (preds == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_dataset)\n",
    "test_acc /= len(test_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(cnn_model.state_dict(), 'cnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fa7a88d-7fd1-4ade-9e09-548d3c1e9e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet cleaning function\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tweet_cleaner(raw_tweet):\n",
    "    cleaned_tweet = re.sub(\"@[A-Za-z0-9]+\", \"\", raw_tweet)\n",
    "    cleaned_tweet = re.sub(\"#\", \"\", cleaned_tweet)\n",
    "    cleaned_tweet = re.sub(r\"http\\S+\", \"\", cleaned_tweet)\n",
    "    cleaned_tweet = re.sub(r\"[^a-zA-Z]\", \" \", cleaned_tweet)\n",
    "    cleaned_tweet = cleaned_tweet.lower().strip()\n",
    "    cleaned_tweet = [token for token in cleaned_tweet.split() if len(token) > 2]\n",
    "    new_sent = ' '.join([lemmatizer.lemmatize(token) for token in cleaned_tweet])\n",
    "    return new_sent.strip()\n",
    "\n",
    "# Prediction function\n",
    "def predict_tweet(model, tweet):\n",
    "    model.eval()\n",
    "    cleaned_tweet = tweet_cleaner(tweet)\n",
    "    seq = text_to_sequence(cleaned_tweet)\n",
    "    padded_seq = pad_sequences([seq], max_length)\n",
    "    input_tensor = torch.LongTensor(padded_seq).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    prediction = (output >= 0.5).float()\n",
    "    return output.item(), prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2c56554-7725-4db9-8779-82857bf41303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 0.3445, Prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "new_tweet = \"this phone is disgusting! #iphone @Apple\"\n",
    "probability, prediction = predict_tweet(cnn_model, new_tweet)\n",
    "print(f\"Probability: {probability:.4f}, Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff41a814-201d-47d1-81a1-4997700428ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a62586d4-e0c5-4d8e-805d-3d7a7ba74332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ComplexCNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, kernel_sizes, max_length):\n",
    "        super(ComplexCNNModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # First set of convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=kernel_sizes[0])\n",
    "        self.bn1 = nn.BatchNorm1d(num_filters)\n",
    "        self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_sizes[1])\n",
    "        self.bn2 = nn.BatchNorm1d(num_filters)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Second set of convolutional layers\n",
    "        self.conv3 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters*2, kernel_size=kernel_sizes[2])\n",
    "        self.bn3 = nn.BatchNorm1d(num_filters*2)\n",
    "        self.conv4 = nn.Conv1d(in_channels=num_filters*2, out_channels=num_filters*2, kernel_size=kernel_sizes[3])\n",
    "        self.bn4 = nn.BatchNorm1d(num_filters*2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Calculate the size of the output from the last pooling layer\n",
    "        self.feature_size = self._get_conv_output_size(max_length)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.feature_size, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, 1)  # Output layer\n",
    "        \n",
    "    def _get_conv_output_size(self, length):\n",
    "        # Helper function to calculate the output size of the last pooling layer\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, self.embedding.embedding_dim, length)\n",
    "            dummy_input = self.conv1(dummy_input)\n",
    "            dummy_input = self.conv2(dummy_input)\n",
    "            dummy_input = self.pool1(dummy_input)\n",
    "            dummy_input = self.conv3(dummy_input)\n",
    "            dummy_input = self.conv4(dummy_input)\n",
    "            dummy_input = self.pool2(dummy_input)\n",
    "            return dummy_input.numel()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(x).permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n",
    "        \n",
    "        # First set of conv layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Second set of conv layers\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return torch.sigmoid(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "636885f9-35c3-43c4-8599-9705fa69d1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplexCNNModel(\n",
      "  (embedding): Embedding(13492, 50, padding_idx=0)\n",
      "  (conv1): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv1d(128, 128, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Epoch [1/5], Loss: 0.5182, Accuracy: 74.31%\n",
      "Epoch [2/5], Loss: 0.3687, Accuracy: 84.26%\n",
      "Epoch [3/5], Loss: 0.2679, Accuracy: 89.27%\n",
      "Epoch [4/5], Loss: 0.1860, Accuracy: 93.59%\n",
      "Epoch [5/5], Loss: 0.1249, Accuracy: 95.82%\n",
      "Test Loss: 0.5643, Test Accuracy: 83.21%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = len(vocab) + 1  # +1 for padding token\n",
    "embedding_dim = 50\n",
    "num_filters = 64\n",
    "kernel_sizes = [3, 3, 3, 3]  # for the four conv layers\n",
    "max_length = 45  # or whatever your max sequence length is\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "complex_cnn_model = ComplexCNNModel(vocab_size, embedding_dim, num_filters, kernel_sizes, max_length).to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(complex_cnn_model)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(complex_cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    complex_cnn_model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = complex_cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        epoch_acc += (preds == labels).sum().item()\n",
    "    \n",
    "    epoch_loss /= len(train_dataset)\n",
    "    epoch_acc /= len(train_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluation\n",
    "complex_cnn_model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = complex_cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        test_acc += (preds == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_dataset)\n",
    "test_acc /= len(test_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(complex_cnn_model.state_dict(), 'complex_cnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a126e5-b0ef-4338-90d1-f7f473428215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "087fe275-c268-4607-b717-e1c54e5bee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.4897, Train Acc: 76.09%, Val Loss: 0.4320, Val Acc: 76.36%\n",
      "Epoch [2/20], Train Loss: 0.3632, Train Acc: 83.14%, Val Loss: 0.3973, Val Acc: 80.56%\n",
      "Epoch [3/20], Train Loss: 0.2899, Train Acc: 87.64%, Val Loss: 0.7976, Val Acc: 80.74%\n",
      "Epoch [4/20], Train Loss: 0.2262, Train Acc: 90.66%, Val Loss: 0.3874, Val Acc: 82.31%\n",
      "Epoch [5/20], Train Loss: 0.1750, Train Acc: 93.43%, Val Loss: 0.5031, Val Acc: 82.14%\n",
      "Epoch [6/20], Train Loss: 0.1152, Train Acc: 95.87%, Val Loss: 0.7345, Val Acc: 81.79%\n",
      "Epoch [7/20], Train Loss: 0.0911, Train Acc: 96.86%, Val Loss: 0.7322, Val Acc: 81.09%\n",
      "Early stopping!\n",
      "Test Loss: 0.4136, Test Accuracy: 81.94%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab) + 1  # +1 for padding token\n",
    "embedding_dim = 50\n",
    "num_filters = 64\n",
    "kernel_sizes = [3, 3, 3, 3]\n",
    "max_length = 45\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-3  # L2 regularization\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ComplexCNNModel(vocab_size, embedding_dim, num_filters, kernel_sizes, max_length).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        train_acc += (preds == labels).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc /= len(train_dataset)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            val_acc += (preds == labels).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_acc /= len(val_dataset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        test_acc += (preds == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_dataset)\n",
    "test_acc /= len(test_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49fbb70-d909-4363-93f2-e7fdf4e455e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ef6ad-c910-412c-9d0b-bd7b30b8888f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae275bc-fc85-4e36-8db5-7f3e35c05d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496256c-a331-4103-af84-82f371b63ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977351fc-2560-45ec-8bdc-4edadd7636e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03282132-39a3-4eac-8516-ae184b1f80b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f188ea-3d6f-492f-8b1c-9c74f2801370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7963c5c0-e5ff-443d-9aa3-d0bbb6bee14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5947, Accuracy: 70.94%\n",
      "Epoch [2/10], Loss: 0.5231, Accuracy: 74.25%\n",
      "Epoch [3/10], Loss: 0.4571, Accuracy: 73.46%\n",
      "Epoch [4/10], Loss: 0.4333, Accuracy: 77.61%\n",
      "Epoch [5/10], Loss: 0.4288, Accuracy: 74.70%\n",
      "Epoch [6/10], Loss: 0.4013, Accuracy: 80.02%\n",
      "Epoch [7/10], Loss: 0.3629, Accuracy: 83.65%\n",
      "Epoch [8/10], Loss: 0.3421, Accuracy: 84.95%\n",
      "Epoch [9/10], Loss: 0.3104, Accuracy: 86.81%\n",
      "Epoch [10/10], Loss: 0.2851, Accuracy: 88.25%\n",
      "Test Loss: 0.3737, Test Accuracy: 83.90%\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=4, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        return torch.sigmoid(self.fc(hidden)).squeeze()\n",
    "\n",
    "# Initialize LSTM model\n",
    "hidden_dim = 100\n",
    "lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim, 1).to(device)\n",
    "\n",
    "# Training loop for LSTM model\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.0005)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        epoch_acc += (preds == labels).sum().item()\n",
    "    \n",
    "    epoch_loss /= len(train_dataset)\n",
    "    epoch_acc /= len(train_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc*100:.2f}%\")\n",
    "\n",
    "# Evaluation of LSTM model\n",
    "lstm_model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        test_acc += (preds == labels).sum().item()\n",
    "\n",
    "test_loss /= len(test_dataset)\n",
    "test_acc /= len(test_dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Save the LSTM model\n",
    "torch.save(lstm_model.state_dict(), 'lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f5eee-78a9-4d50-a3d4-33a4e91fc08a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
